paper_id,Summary,Questions,Limitations,Ethical Concerns,Soundness,Presentation,Contribution,Overall,Confidence,Strengths,Weaknesses,Originality,Quality,Clarity,Significance,Decision
DualScale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models,"This paper proposes an adaptive dual-scale denoising architecture for low-dimensional (2D) diffusion models. The model consists of two parallel MLP branches—one processing the original (global) features and the other an upscaled (local) version—whose outputs are combined via a learnable, timestep-conditioned weighting mechanism. The approach is evaluated on four synthetic 2D datasets, showing modest improvements in KL divergence and visual sample quality compared to a single-branch baseline. Ablation studies and analysis of the learned weights are provided.","['Can the authors provide theoretical or empirical motivation for why dual-scale adaptive weighting should help in diffusion models, especially in low dimensions?', 'Why is the evaluation limited to synthetic 2D datasets? Have the authors attempted to apply the method to higher-dimensional or real-world data?', 'How does the method compare to a single-branch MLP of equivalent or higher capacity, or to other adaptive feature fusion techniques (e.g., attention, gating, hierarchical models)?', 'Is the upscaling operation critical? What happens if the local branch simply reprocesses the original input, or if the upscaled dimension is varied?', 'Are the reported KL improvements statistically significant and consistent across different random seeds?', 'Can the authors clarify the precise upscaling operation and justify the choice of projection dimension?']","['The method is only evaluated on simple 2D synthetic datasets; its applicability to real-world or higher-dimensional data is untested.', 'The computational overhead is significant relative to the modest performance improvement.', 'No theoretical understanding or insight is offered for when dual-scale adaptive weighting is necessary or effective.', 'The adaptive weighting mechanism may overfit to the specific datasets and may not generalize.', 'No discussion of potential negative societal impacts or misuse, though this is less relevant for toy generative modeling.']",False,2,3,2,2,4,"['The paper is clearly written and well-organized, with explicit architectural descriptions.', 'The proposed adaptive weighting mechanism is simple, interpretable, and easy to implement.', 'Experiments are reproducible, with code availability and detailed experimental settings.', 'Ablation studies and analysis of the weighting mechanism are included.']","['Technical novelty is minimal: the architecture is a straightforward combination of two MLP branches with learnable weighting, an established idea in deep learning.', 'No theoretical justification or principled motivation is provided for why adaptive dual-scale processing is beneficial in diffusion models.', 'Empirical evaluation is limited to trivial synthetic 2D datasets; there is no evidence the method generalizes to real or higher-dimensional data.', 'Reported improvements are modest (2-13% KL reduction) and may not be practically significant, especially given the doubled computational cost.', 'No comparison to stronger or more sophisticated baselines (e.g., attention, hierarchical, or multi-scale architectures used in diffusion models).', 'No discussion of negative results, failure cases, or broader applicability.', 'The work does not advance the state of the art in generative modeling in any substantive way.']",2,2,3,1,Reject