paper_id,Summary,Questions,Limitations,Ethical Concerns,Soundness,Presentation,Contribution,Overall,Confidence,Strengths,Weaknesses,Originality,Quality,Clarity,Significance,Decision
Unlocking Grokking: A Comparative Study of Weight Initialization Strategies in Transformer Models,"This paper presents a systematic empirical study of the impact of five standard weight initialization strategies (PyTorch default, Xavier, He, Orthogonal, Kaiming Normal) on the grokking phenomenon in small Transformer models trained on simple arithmetic tasks in finite fields. The main finding is that Xavier and Orthogonal initializations lead to faster and more reliable grokking (measured as steps to 99% validation accuracy) compared to other initializations. The study is limited to small models and toy tasks, and is entirely empirical.","['Can the authors provide a mechanistic or theoretical explanation for why certain initialization strategies facilitate grokking, beyond empirical observations?', 'Do the observed effects generalize to larger Transformer models or to real-world tasks?', 'Why were only three random seeds used? Can results be replicated with more seeds to ensure robustness?', 'Can the authors clarify the missing figures and provide code or detailed pseudocode for full reproducibility?', 'Are there broader implications or actionable insights for the deep learning community beyond the specific toy setting studied?']","['Results are limited to small models and simple tasks; findings may not generalize to larger models or practical domains.', 'No theoretical or mechanistic explanation for the observed differences between initialization strategies.', 'Statistical robustness is limited by the use of only three seeds.', 'No code or resource release, limiting reproducibility.', 'No discussion of societal impact, ethical considerations, or broader limitations.']",False,2,2,1,2,4,"['Addresses the timely and interesting phenomenon of grokking in deep learning.', 'Systematic and controlled empirical comparison of multiple initialization strategies.', 'Clear experimental setup, with reporting of statistical measures (mean, confidence intervals).', 'Experimental protocols are described in detail, aiding reproducibility in principle.']","['The contribution is narrow and highly incremental, amounting to an empirical sweep of standard initialization methods with no new theory, methodology, or deep analysis.', 'Findings are unsurprising and align with established knowledge that initialization affects optimization and convergence.', 'Experiments are limited to very small models and synthetic arithmetic tasks; there is no evidence for generality to larger models or real-world tasks.', 'Only three random seeds are used per configuration, limiting statistical robustness.', 'No mechanistic or theoretical explanation is provided for why certain initializations facilitate grokking.', 'Figures are missing from the submission, and the presentation is incomplete.', 'No code or resources are provided for reproducibility.', 'Related work is only superficially covered, and connections to recent advances in grokking theory or practice are lacking.', 'No discussion of broader implications, limitations, or societal impact.']",1,2,3,1,Reject