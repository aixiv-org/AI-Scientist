title: UNLOCKING GROKKING: A COMPARATIVE STUDY OF WEIGHT INITIALIZATION STRATEGIES IN TRANS- FORMER MODELS
abstract: This paper investigates the impact of weight initialization strategies on the grokking phenomenon in Transformer models, addressing the challenge of understanding and optimizing neural network learning dynamics. Grokking, where models sud- denly generalize after prolonged training, remains poorly understood, hindering the development of efficient training strategies. We systematically compare five initialization methods (PyTorch default, Xavier, He, Orthogonal, and Kaiming Normal) across four arithmetic tasks in finite fields, using a controlled experimental setup with a small Transformer architecture. Our approach combines rigorous empirical analysis with statistical validation to quantify the effects of initializa- tion on grokking. Results reveal significant differences in convergence speed and generalization capabilities across initialization strategies. Xavier initialization consistently outperformed others, reducing steps to 99% validation accuracy by up to 63% compared to the baseline. Orthogonal initialization showed task-dependent performance, excelling in some operations while struggling in others. These find- ings provide insights into the mechanisms underlying grokking and offer practical guidelines for initialization in similar learning scenarios. Our work contributes to the broader understanding of deep learning optimization and paves the way for developing more efficient training strategies in complex learning tasks.