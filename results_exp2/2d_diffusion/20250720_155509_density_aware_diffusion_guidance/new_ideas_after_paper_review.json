[
    {
        "Name": "hybrid_adaptive_embeddings",
        "Title": "Hybrid Adaptive Embeddings for Low-Dimensional Diffusion Models",
        "Problem Statement": "Current fixed embeddings don't optimally adapt to both the input data geometry and diffusion timestep dynamics.",
        "Motivation": "The interaction between input features and diffusion timesteps could benefit from learned, adaptive components while retaining the benefits of sinusoidal embeddings.",
        "Proposed Method": "Create hybrid embeddings that combine fixed sinusoidal patterns with learned MLP-based modulation terms conditioned on both input features and timestep.",
        "Experiment": "1) Design new embedding layer that combines sinusoidal basis with learned modulation. 2) Keep core architecture identical for fair comparison. 3) Evaluate on all datasets using same metrics. 4) Analyze how modulation patterns vary across timesteps.",
        "Interestingness": 8,
        "Feasibility": 9,
        "Novelty": 7,
        "lit_review_results": "title: Denoising diffusion probabilistic models are optimally adaptive to   unknown low dimensionality\nabstract: The denoising diffusion probabilistic model (DDPM) has emerged as a mainstream generative model in generative AI. While sharp convergence guarantees have been established for the DDPM, the iteration complexity is, in general, proportional to the ambient data dimension, resulting in overly conservative theory that fails to explain its practical efficiency. This has motivated the recent work Li and Yan (2024a) to investigate how the DDPM can achieve sampling speed-ups through automatic exploitation of intrinsic low dimensionality of data. We strengthen this line of work by demonstrating, in some sense, optimal adaptivity to unknown low dimensionality. For a broad class of data distributions with intrinsic dimension $k$, we prove that the iteration complexity of the DDPM scales nearly linearly with $k$, which is optimal when using KL divergence to measure distributional discrepancy. Notably, our work is closely aligned with the independent concurrent work Potaptchik et al. (2024) -- posted two weeks prior to ours -- in establishing nearly linear-$k$ convergence guarantees for the DDPM.\n\ntitle: Denoising Diffusion Probabilistic Models in Six Simple Steps\nabstract: Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but they have a high barrier-to-entry as they require background knowledge of stochastic differential equations and probability flow. In this note, we distill down the formulation of the DDPM into six simple steps each of which comes with a clear rationale. We assume that the reader is familiar with fundamental topics in machine learning including basic probabilistic modelling, Gaussian distributions, maximum likelihood estimation, and deep learning.\n\ntitle: Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion   Models\nabstract: This paper investigates score-based diffusion models when the underlying target distribution is concentrated on or near low-dimensional manifolds within the higher-dimensional space in which they formally reside, a common characteristic of natural image distributions. Despite previous efforts to understand the data generation process of diffusion models, existing theoretical support remains highly suboptimal in the presence of low-dimensional structure, which we strengthen in this paper. For the popular Denoising Diffusion Probabilistic Model (DDPM), we find that the dependency of the error incurred within each denoising step on the ambient dimension $d$ is in general unavoidable. We further identify a unique design of coefficients that yields a converges rate at the order of $O(k^{2}/\\sqrt{T})$ (up to log factors), where $k$ is the intrinsic dimension of the target distribution and $T$ is the number of steps. This represents the first theoretical demonstration that the DDPM sampler can adapt to unknown low-dimensional structures in the target distribution, highlighting the critical importance of coefficient design. All of this is achieved by a novel set of analysis tools that characterize the algorithmic dynamics in a more deterministic manner.\n\ntitle: Structured Denoising Diffusion Models in Discrete State-Spaces\nabstract: Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.\n\ntitle: Improved Denoising Diffusion Probabilistic Models\nabstract: Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion\n\n",
        "paper": "title: DUALSCALE DIFFUSION: ADAPTIVE FEATURE BAL- ANCING FOR LOW-DIMENSIONAL GENERATIVE MOD- ELS\nabstract: This paper introduces an adaptive dual-scale denoising approach for low- dimensional diffusion models, addressing the challenge of balancing global struc- ture and local detail in generated samples. While diffusion models have shown re- markable success in high-dimensional spaces, their application to low-dimensional data remains crucial for understanding fundamental model behaviors and address- ing real-world applications with inherently low-dimensional data. However, in these spaces, traditional models often struggle to simultaneously capture both macro-level patterns and fine-grained features, leading to suboptimal sample qual- ity. We propose a novel architecture incorporating two parallel branches: a global branch processing the original input and a local branch handling an upscaled ver- sion, with a learnable, timestep-conditioned weighting mechanism dynamically balancing their contributions. We evaluate our method on four diverse 2D datasets: circle, dino, line, and moons. Our results demonstrate significant improvements in sample quality, with KL divergence reductions of up to 12.8% compared to the baseline model. The adaptive weighting successfully adjusts the focus be- tween global and local features across different datasets and denoising stages, as evidenced by our weight evolution analysis. This work not only enhances low-dimensional diffusion models but also provides insights that could inform improvements in higher-dimensional domains, opening new avenues for advancing generative modeling across various applications.",
        "paper_review_results": "{\"paper_id\": \"DualScale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models\", \"Summary\": \"This paper proposes an adaptive dual-scale denoising architecture for low-dimensional (2D) diffusion models. The model consists of two parallel MLP branches\\u2014one processing the original (global) features and the other an upscaled (local) version\\u2014whose outputs are combined via a learnable, timestep-conditioned weighting mechanism. The approach is evaluated on four synthetic 2D datasets, showing modest improvements in KL divergence and visual sample quality compared to a single-branch baseline. Ablation studies and analysis of the learned weights are provided.\", \"Questions\": \"['Can the authors provide theoretical or empirical motivation for why dual-scale adaptive weighting should help in diffusion models, especially in low dimensions?', 'Why is the evaluation limited to synthetic 2D datasets? Have the authors attempted to apply the method to higher-dimensional or real-world data?', 'How does the method compare to a single-branch MLP of equivalent or higher capacity, or to other adaptive feature fusion techniques (e.g., attention, gating, hierarchical models)?', 'Is the upscaling operation critical? What happens if the local branch simply reprocesses the original input, or if the upscaled dimension is varied?', 'Are the reported KL improvements statistically significant and consistent across different random seeds?', 'Can the authors clarify the precise upscaling operation and justify the choice of projection dimension?']\", \"Limitations\": \"['The method is only evaluated on simple 2D synthetic datasets; its applicability to real-world or higher-dimensional data is untested.', 'The computational overhead is significant relative to the modest performance improvement.', 'No theoretical understanding or insight is offered for when dual-scale adaptive weighting is necessary or effective.', 'The adaptive weighting mechanism may overfit to the specific datasets and may not generalize.', 'No discussion of potential negative societal impacts or misuse, though this is less relevant for toy generative modeling.']\", \"Ethical Concerns\": false, \"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Overall\": 2, \"Confidence\": 4, \"Strengths\": \"['The paper is clearly written and well-organized, with explicit architectural descriptions.', 'The proposed adaptive weighting mechanism is simple, interpretable, and easy to implement.', 'Experiments are reproducible, with code availability and detailed experimental settings.', 'Ablation studies and analysis of the weighting mechanism are included.']\", \"Weaknesses\": \"['Technical novelty is minimal: the architecture is a straightforward combination of two MLP branches with learnable weighting, an established idea in deep learning.', 'No theoretical justification or principled motivation is provided for why adaptive dual-scale processing is beneficial in diffusion models.', 'Empirical evaluation is limited to trivial synthetic 2D datasets; there is no evidence the method generalizes to real or higher-dimensional data.', 'Reported improvements are modest (2-13% KL reduction) and may not be practically significant, especially given the doubled computational cost.', 'No comparison to stronger or more sophisticated baselines (e.g., attention, hierarchical, or multi-scale architectures used in diffusion models).', 'No discussion of negative results, failure cases, or broader applicability.', 'The work does not advance the state of the art in generative modeling in any substantive way.']\", \"Originality\": 2, \"Quality\": 2, \"Clarity\": 3, \"Significance\": 1, \"Decision\": \"Reject\"}",
        "novel": true
    },
    {
        "Name": "timestep_adaptive_scaling",
        "Title": "Timestep-Adaptive Embedding Scaling for Diffusion Models",
        "Problem Statement": "Fixed-scale sinusoidal embeddings in diffusion models cannot adjust their frequency content based on the denoising stage, potentially limiting their ability to capture both global and local features optimally.",
        "Motivation": "Different denoising stages require different frequency content in embeddings - early steps benefit from low-frequency global information while later steps need high-frequency local details.",
        "Proposed Method": "Modify the SinusoidalEmbedding class to learn timestep-dependent scale parameters, allowing automatic adjustment of embedding frequencies throughout the denoising process.",
        "Experiment": "1) Extend SinusoidalEmbedding to learn a scale parameter per timestep. 2) Compare with fixed-scale baseline on all datasets. 3) Analyze how learned scales evolve across timesteps. 4) Measure impact on sample quality and KL divergence.",
        "Interestingness": 8,
        "Feasibility": 9,
        "Novelty": 7,
        "lit_review_results": "title: Denoising diffusion probabilistic models are optimally adaptive to   unknown low dimensionality\nabstract: The denoising diffusion probabilistic model (DDPM) has emerged as a mainstream generative model in generative AI. While sharp convergence guarantees have been established for the DDPM, the iteration complexity is, in general, proportional to the ambient data dimension, resulting in overly conservative theory that fails to explain its practical efficiency. This has motivated the recent work Li and Yan (2024a) to investigate how the DDPM can achieve sampling speed-ups through automatic exploitation of intrinsic low dimensionality of data. We strengthen this line of work by demonstrating, in some sense, optimal adaptivity to unknown low dimensionality. For a broad class of data distributions with intrinsic dimension $k$, we prove that the iteration complexity of the DDPM scales nearly linearly with $k$, which is optimal when using KL divergence to measure distributional discrepancy. Notably, our work is closely aligned with the independent concurrent work Potaptchik et al. (2024) -- posted two weeks prior to ours -- in establishing nearly linear-$k$ convergence guarantees for the DDPM.\n\ntitle: Denoising Diffusion Probabilistic Models in Six Simple Steps\nabstract: Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but they have a high barrier-to-entry as they require background knowledge of stochastic differential equations and probability flow. In this note, we distill down the formulation of the DDPM into six simple steps each of which comes with a clear rationale. We assume that the reader is familiar with fundamental topics in machine learning including basic probabilistic modelling, Gaussian distributions, maximum likelihood estimation, and deep learning.\n\ntitle: Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion   Models\nabstract: This paper investigates score-based diffusion models when the underlying target distribution is concentrated on or near low-dimensional manifolds within the higher-dimensional space in which they formally reside, a common characteristic of natural image distributions. Despite previous efforts to understand the data generation process of diffusion models, existing theoretical support remains highly suboptimal in the presence of low-dimensional structure, which we strengthen in this paper. For the popular Denoising Diffusion Probabilistic Model (DDPM), we find that the dependency of the error incurred within each denoising step on the ambient dimension $d$ is in general unavoidable. We further identify a unique design of coefficients that yields a converges rate at the order of $O(k^{2}/\\sqrt{T})$ (up to log factors), where $k$ is the intrinsic dimension of the target distribution and $T$ is the number of steps. This represents the first theoretical demonstration that the DDPM sampler can adapt to unknown low-dimensional structures in the target distribution, highlighting the critical importance of coefficient design. All of this is achieved by a novel set of analysis tools that characterize the algorithmic dynamics in a more deterministic manner.\n\ntitle: Structured Denoising Diffusion Models in Discrete State-Spaces\nabstract: Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.\n\ntitle: Improved Denoising Diffusion Probabilistic Models\nabstract: Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion\n\n",
        "paper": "title: DUALSCALE DIFFUSION: ADAPTIVE FEATURE BAL- ANCING FOR LOW-DIMENSIONAL GENERATIVE MOD- ELS\nabstract: This paper introduces an adaptive dual-scale denoising approach for low- dimensional diffusion models, addressing the challenge of balancing global struc- ture and local detail in generated samples. While diffusion models have shown re- markable success in high-dimensional spaces, their application to low-dimensional data remains crucial for understanding fundamental model behaviors and address- ing real-world applications with inherently low-dimensional data. However, in these spaces, traditional models often struggle to simultaneously capture both macro-level patterns and fine-grained features, leading to suboptimal sample qual- ity. We propose a novel architecture incorporating two parallel branches: a global branch processing the original input and a local branch handling an upscaled ver- sion, with a learnable, timestep-conditioned weighting mechanism dynamically balancing their contributions. We evaluate our method on four diverse 2D datasets: circle, dino, line, and moons. Our results demonstrate significant improvements in sample quality, with KL divergence reductions of up to 12.8% compared to the baseline model. The adaptive weighting successfully adjusts the focus be- tween global and local features across different datasets and denoising stages, as evidenced by our weight evolution analysis. This work not only enhances low-dimensional diffusion models but also provides insights that could inform improvements in higher-dimensional domains, opening new avenues for advancing generative modeling across various applications.",
        "paper_review_results": "{\"paper_id\": \"DualScale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models\", \"Summary\": \"This paper proposes an adaptive dual-scale denoising architecture for low-dimensional (2D) diffusion models. The model consists of two parallel MLP branches\\u2014one processing the original (global) features and the other an upscaled (local) version\\u2014whose outputs are combined via a learnable, timestep-conditioned weighting mechanism. The approach is evaluated on four synthetic 2D datasets, showing modest improvements in KL divergence and visual sample quality compared to a single-branch baseline. Ablation studies and analysis of the learned weights are provided.\", \"Questions\": \"['Can the authors provide theoretical or empirical motivation for why dual-scale adaptive weighting should help in diffusion models, especially in low dimensions?', 'Why is the evaluation limited to synthetic 2D datasets? Have the authors attempted to apply the method to higher-dimensional or real-world data?', 'How does the method compare to a single-branch MLP of equivalent or higher capacity, or to other adaptive feature fusion techniques (e.g., attention, gating, hierarchical models)?', 'Is the upscaling operation critical? What happens if the local branch simply reprocesses the original input, or if the upscaled dimension is varied?', 'Are the reported KL improvements statistically significant and consistent across different random seeds?', 'Can the authors clarify the precise upscaling operation and justify the choice of projection dimension?']\", \"Limitations\": \"['The method is only evaluated on simple 2D synthetic datasets; its applicability to real-world or higher-dimensional data is untested.', 'The computational overhead is significant relative to the modest performance improvement.', 'No theoretical understanding or insight is offered for when dual-scale adaptive weighting is necessary or effective.', 'The adaptive weighting mechanism may overfit to the specific datasets and may not generalize.', 'No discussion of potential negative societal impacts or misuse, though this is less relevant for toy generative modeling.']\", \"Ethical Concerns\": false, \"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Overall\": 2, \"Confidence\": 4, \"Strengths\": \"['The paper is clearly written and well-organized, with explicit architectural descriptions.', 'The proposed adaptive weighting mechanism is simple, interpretable, and easy to implement.', 'Experiments are reproducible, with code availability and detailed experimental settings.', 'Ablation studies and analysis of the weighting mechanism are included.']\", \"Weaknesses\": \"['Technical novelty is minimal: the architecture is a straightforward combination of two MLP branches with learnable weighting, an established idea in deep learning.', 'No theoretical justification or principled motivation is provided for why adaptive dual-scale processing is beneficial in diffusion models.', 'Empirical evaluation is limited to trivial synthetic 2D datasets; there is no evidence the method generalizes to real or higher-dimensional data.', 'Reported improvements are modest (2-13% KL reduction) and may not be practically significant, especially given the doubled computational cost.', 'No comparison to stronger or more sophisticated baselines (e.g., attention, hierarchical, or multi-scale architectures used in diffusion models).', 'No discussion of negative results, failure cases, or broader applicability.', 'The work does not advance the state of the art in generative modeling in any substantive way.']\", \"Originality\": 2, \"Quality\": 2, \"Clarity\": 3, \"Significance\": 1, \"Decision\": \"Reject\"}",
        "novel": true
    },
    {
        "Name": "learnable_noise_schedule",
        "Title": "Data-Adaptive Learnable Noise Schedules for Diffusion Models",
        "Problem Statement": "Fixed noise schedules in diffusion models may not optimally adapt to different data distributions, potentially limiting denoising efficiency and sample quality.",
        "Motivation": "Different datasets have varying geometric and topological properties that may benefit from customized noise reduction trajectories during denoising. Learning schedule parameters directly from data could automatically discover optimal noise profiles.",
        "Proposed Method": "Modify the NoiseScheduler to learn schedule parameters (betas) via gradient descent while maintaining the diffusion process constraints. The schedule will be parameterized as a monotonic neural network conditioned on timestep and optionally dataset features.",
        "Experiment": "1) Implement learnable schedule parameterization in NoiseScheduler. 2) Train with same architecture but learnable schedule. 3) Compare against fixed schedules on all datasets. 4) Analyze learned schedule patterns across datasets. 5) Measure impact on sample quality (KL divergence) and training stability.",
        "Interestingness": 8,
        "Feasibility": 9,
        "Novelty": 7,
        "lit_review_results": "title: Denoising diffusion probabilistic models are optimally adaptive to   unknown low dimensionality\nabstract: The denoising diffusion probabilistic model (DDPM) has emerged as a mainstream generative model in generative AI. While sharp convergence guarantees have been established for the DDPM, the iteration complexity is, in general, proportional to the ambient data dimension, resulting in overly conservative theory that fails to explain its practical efficiency. This has motivated the recent work Li and Yan (2024a) to investigate how the DDPM can achieve sampling speed-ups through automatic exploitation of intrinsic low dimensionality of data. We strengthen this line of work by demonstrating, in some sense, optimal adaptivity to unknown low dimensionality. For a broad class of data distributions with intrinsic dimension $k$, we prove that the iteration complexity of the DDPM scales nearly linearly with $k$, which is optimal when using KL divergence to measure distributional discrepancy. Notably, our work is closely aligned with the independent concurrent work Potaptchik et al. (2024) -- posted two weeks prior to ours -- in establishing nearly linear-$k$ convergence guarantees for the DDPM.\n\ntitle: Denoising Diffusion Probabilistic Models in Six Simple Steps\nabstract: Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but they have a high barrier-to-entry as they require background knowledge of stochastic differential equations and probability flow. In this note, we distill down the formulation of the DDPM into six simple steps each of which comes with a clear rationale. We assume that the reader is familiar with fundamental topics in machine learning including basic probabilistic modelling, Gaussian distributions, maximum likelihood estimation, and deep learning.\n\ntitle: Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion   Models\nabstract: This paper investigates score-based diffusion models when the underlying target distribution is concentrated on or near low-dimensional manifolds within the higher-dimensional space in which they formally reside, a common characteristic of natural image distributions. Despite previous efforts to understand the data generation process of diffusion models, existing theoretical support remains highly suboptimal in the presence of low-dimensional structure, which we strengthen in this paper. For the popular Denoising Diffusion Probabilistic Model (DDPM), we find that the dependency of the error incurred within each denoising step on the ambient dimension $d$ is in general unavoidable. We further identify a unique design of coefficients that yields a converges rate at the order of $O(k^{2}/\\sqrt{T})$ (up to log factors), where $k$ is the intrinsic dimension of the target distribution and $T$ is the number of steps. This represents the first theoretical demonstration that the DDPM sampler can adapt to unknown low-dimensional structures in the target distribution, highlighting the critical importance of coefficient design. All of this is achieved by a novel set of analysis tools that characterize the algorithmic dynamics in a more deterministic manner.\n\ntitle: Structured Denoising Diffusion Models in Discrete State-Spaces\nabstract: Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.\n\ntitle: Improved Denoising Diffusion Probabilistic Models\nabstract: Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion\n\n",
        "paper": "title: DUALSCALE DIFFUSION: ADAPTIVE FEATURE BAL- ANCING FOR LOW-DIMENSIONAL GENERATIVE MOD- ELS\nabstract: This paper introduces an adaptive dual-scale denoising approach for low- dimensional diffusion models, addressing the challenge of balancing global struc- ture and local detail in generated samples. While diffusion models have shown re- markable success in high-dimensional spaces, their application to low-dimensional data remains crucial for understanding fundamental model behaviors and address- ing real-world applications with inherently low-dimensional data. However, in these spaces, traditional models often struggle to simultaneously capture both macro-level patterns and fine-grained features, leading to suboptimal sample qual- ity. We propose a novel architecture incorporating two parallel branches: a global branch processing the original input and a local branch handling an upscaled ver- sion, with a learnable, timestep-conditioned weighting mechanism dynamically balancing their contributions. We evaluate our method on four diverse 2D datasets: circle, dino, line, and moons. Our results demonstrate significant improvements in sample quality, with KL divergence reductions of up to 12.8% compared to the baseline model. The adaptive weighting successfully adjusts the focus be- tween global and local features across different datasets and denoising stages, as evidenced by our weight evolution analysis. This work not only enhances low-dimensional diffusion models but also provides insights that could inform improvements in higher-dimensional domains, opening new avenues for advancing generative modeling across various applications.",
        "paper_review_results": "{\"paper_id\": \"DualScale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models\", \"Summary\": \"This paper proposes an adaptive dual-scale denoising architecture for low-dimensional (2D) diffusion models. The model consists of two parallel MLP branches\\u2014one processing the original (global) features and the other an upscaled (local) version\\u2014whose outputs are combined via a learnable, timestep-conditioned weighting mechanism. The approach is evaluated on four synthetic 2D datasets, showing modest improvements in KL divergence and visual sample quality compared to a single-branch baseline. Ablation studies and analysis of the learned weights are provided.\", \"Questions\": \"['Can the authors provide theoretical or empirical motivation for why dual-scale adaptive weighting should help in diffusion models, especially in low dimensions?', 'Why is the evaluation limited to synthetic 2D datasets? Have the authors attempted to apply the method to higher-dimensional or real-world data?', 'How does the method compare to a single-branch MLP of equivalent or higher capacity, or to other adaptive feature fusion techniques (e.g., attention, gating, hierarchical models)?', 'Is the upscaling operation critical? What happens if the local branch simply reprocesses the original input, or if the upscaled dimension is varied?', 'Are the reported KL improvements statistically significant and consistent across different random seeds?', 'Can the authors clarify the precise upscaling operation and justify the choice of projection dimension?']\", \"Limitations\": \"['The method is only evaluated on simple 2D synthetic datasets; its applicability to real-world or higher-dimensional data is untested.', 'The computational overhead is significant relative to the modest performance improvement.', 'No theoretical understanding or insight is offered for when dual-scale adaptive weighting is necessary or effective.', 'The adaptive weighting mechanism may overfit to the specific datasets and may not generalize.', 'No discussion of potential negative societal impacts or misuse, though this is less relevant for toy generative modeling.']\", \"Ethical Concerns\": false, \"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Overall\": 2, \"Confidence\": 4, \"Strengths\": \"['The paper is clearly written and well-organized, with explicit architectural descriptions.', 'The proposed adaptive weighting mechanism is simple, interpretable, and easy to implement.', 'Experiments are reproducible, with code availability and detailed experimental settings.', 'Ablation studies and analysis of the weighting mechanism are included.']\", \"Weaknesses\": \"['Technical novelty is minimal: the architecture is a straightforward combination of two MLP branches with learnable weighting, an established idea in deep learning.', 'No theoretical justification or principled motivation is provided for why adaptive dual-scale processing is beneficial in diffusion models.', 'Empirical evaluation is limited to trivial synthetic 2D datasets; there is no evidence the method generalizes to real or higher-dimensional data.', 'Reported improvements are modest (2-13% KL reduction) and may not be practically significant, especially given the doubled computational cost.', 'No comparison to stronger or more sophisticated baselines (e.g., attention, hierarchical, or multi-scale architectures used in diffusion models).', 'No discussion of negative results, failure cases, or broader applicability.', 'The work does not advance the state of the art in generative modeling in any substantive way.']\", \"Originality\": 2, \"Quality\": 2, \"Clarity\": 3, \"Significance\": 1, \"Decision\": \"Reject\"}",
        "novel": false
    },
    {
        "Name": "controllable_diffusion_guidance",
        "Title": "Controllable Generation in Diffusion Models via Gradient Guidance",
        "Problem Statement": "Diffusion models lack built-in mechanisms for controlling sample generation towards specific regions or properties of the data space.",
        "Motivation": "Practical applications often require generating samples with specific characteristics while maintaining overall data fidelity. A lightweight control method that doesn't require retraining would be valuable.",
        "Proposed Method": "During sampling, modify denoising steps by adding gradients from constraint functions. Implement both hard constraints (via projection) and soft constraints (via gradient weighting) with adjustable strength parameters.",
        "Experiment": "1) Define constraint functions for each dataset (quadrant, cluster membership). 2) Modify NoiseScheduler.step() to apply constraints. 3) Evaluate: a) Constraint satisfaction rate b) Sample quality (KL) vs guidance strength c) Comparison to rejection sampling baseline.",
        "Interestingness": 9,
        "Feasibility": 9,
        "Novelty": 8,
        "lit_review_results": "title: Denoising diffusion probabilistic models are optimally adaptive to   unknown low dimensionality\nabstract: The denoising diffusion probabilistic model (DDPM) has emerged as a mainstream generative model in generative AI. While sharp convergence guarantees have been established for the DDPM, the iteration complexity is, in general, proportional to the ambient data dimension, resulting in overly conservative theory that fails to explain its practical efficiency. This has motivated the recent work Li and Yan (2024a) to investigate how the DDPM can achieve sampling speed-ups through automatic exploitation of intrinsic low dimensionality of data. We strengthen this line of work by demonstrating, in some sense, optimal adaptivity to unknown low dimensionality. For a broad class of data distributions with intrinsic dimension $k$, we prove that the iteration complexity of the DDPM scales nearly linearly with $k$, which is optimal when using KL divergence to measure distributional discrepancy. Notably, our work is closely aligned with the independent concurrent work Potaptchik et al. (2024) -- posted two weeks prior to ours -- in establishing nearly linear-$k$ convergence guarantees for the DDPM.\n\ntitle: Denoising Diffusion Probabilistic Models in Six Simple Steps\nabstract: Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but they have a high barrier-to-entry as they require background knowledge of stochastic differential equations and probability flow. In this note, we distill down the formulation of the DDPM into six simple steps each of which comes with a clear rationale. We assume that the reader is familiar with fundamental topics in machine learning including basic probabilistic modelling, Gaussian distributions, maximum likelihood estimation, and deep learning.\n\ntitle: Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion   Models\nabstract: This paper investigates score-based diffusion models when the underlying target distribution is concentrated on or near low-dimensional manifolds within the higher-dimensional space in which they formally reside, a common characteristic of natural image distributions. Despite previous efforts to understand the data generation process of diffusion models, existing theoretical support remains highly suboptimal in the presence of low-dimensional structure, which we strengthen in this paper. For the popular Denoising Diffusion Probabilistic Model (DDPM), we find that the dependency of the error incurred within each denoising step on the ambient dimension $d$ is in general unavoidable. We further identify a unique design of coefficients that yields a converges rate at the order of $O(k^{2}/\\sqrt{T})$ (up to log factors), where $k$ is the intrinsic dimension of the target distribution and $T$ is the number of steps. This represents the first theoretical demonstration that the DDPM sampler can adapt to unknown low-dimensional structures in the target distribution, highlighting the critical importance of coefficient design. All of this is achieved by a novel set of analysis tools that characterize the algorithmic dynamics in a more deterministic manner.\n\ntitle: Structured Denoising Diffusion Models in Discrete State-Spaces\nabstract: Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.\n\ntitle: Improved Denoising Diffusion Probabilistic Models\nabstract: Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion\n\n",
        "paper": "title: DUALSCALE DIFFUSION: ADAPTIVE FEATURE BAL- ANCING FOR LOW-DIMENSIONAL GENERATIVE MOD- ELS\nabstract: This paper introduces an adaptive dual-scale denoising approach for low- dimensional diffusion models, addressing the challenge of balancing global struc- ture and local detail in generated samples. While diffusion models have shown re- markable success in high-dimensional spaces, their application to low-dimensional data remains crucial for understanding fundamental model behaviors and address- ing real-world applications with inherently low-dimensional data. However, in these spaces, traditional models often struggle to simultaneously capture both macro-level patterns and fine-grained features, leading to suboptimal sample qual- ity. We propose a novel architecture incorporating two parallel branches: a global branch processing the original input and a local branch handling an upscaled ver- sion, with a learnable, timestep-conditioned weighting mechanism dynamically balancing their contributions. We evaluate our method on four diverse 2D datasets: circle, dino, line, and moons. Our results demonstrate significant improvements in sample quality, with KL divergence reductions of up to 12.8% compared to the baseline model. The adaptive weighting successfully adjusts the focus be- tween global and local features across different datasets and denoising stages, as evidenced by our weight evolution analysis. This work not only enhances low-dimensional diffusion models but also provides insights that could inform improvements in higher-dimensional domains, opening new avenues for advancing generative modeling across various applications.",
        "paper_review_results": "{\"paper_id\": \"DualScale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models\", \"Summary\": \"This paper proposes an adaptive dual-scale denoising architecture for low-dimensional (2D) diffusion models. The model consists of two parallel MLP branches\\u2014one processing the original (global) features and the other an upscaled (local) version\\u2014whose outputs are combined via a learnable, timestep-conditioned weighting mechanism. The approach is evaluated on four synthetic 2D datasets, showing modest improvements in KL divergence and visual sample quality compared to a single-branch baseline. Ablation studies and analysis of the learned weights are provided.\", \"Questions\": \"['Can the authors provide theoretical or empirical motivation for why dual-scale adaptive weighting should help in diffusion models, especially in low dimensions?', 'Why is the evaluation limited to synthetic 2D datasets? Have the authors attempted to apply the method to higher-dimensional or real-world data?', 'How does the method compare to a single-branch MLP of equivalent or higher capacity, or to other adaptive feature fusion techniques (e.g., attention, gating, hierarchical models)?', 'Is the upscaling operation critical? What happens if the local branch simply reprocesses the original input, or if the upscaled dimension is varied?', 'Are the reported KL improvements statistically significant and consistent across different random seeds?', 'Can the authors clarify the precise upscaling operation and justify the choice of projection dimension?']\", \"Limitations\": \"['The method is only evaluated on simple 2D synthetic datasets; its applicability to real-world or higher-dimensional data is untested.', 'The computational overhead is significant relative to the modest performance improvement.', 'No theoretical understanding or insight is offered for when dual-scale adaptive weighting is necessary or effective.', 'The adaptive weighting mechanism may overfit to the specific datasets and may not generalize.', 'No discussion of potential negative societal impacts or misuse, though this is less relevant for toy generative modeling.']\", \"Ethical Concerns\": false, \"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Overall\": 2, \"Confidence\": 4, \"Strengths\": \"['The paper is clearly written and well-organized, with explicit architectural descriptions.', 'The proposed adaptive weighting mechanism is simple, interpretable, and easy to implement.', 'Experiments are reproducible, with code availability and detailed experimental settings.', 'Ablation studies and analysis of the weighting mechanism are included.']\", \"Weaknesses\": \"['Technical novelty is minimal: the architecture is a straightforward combination of two MLP branches with learnable weighting, an established idea in deep learning.', 'No theoretical justification or principled motivation is provided for why adaptive dual-scale processing is beneficial in diffusion models.', 'Empirical evaluation is limited to trivial synthetic 2D datasets; there is no evidence the method generalizes to real or higher-dimensional data.', 'Reported improvements are modest (2-13% KL reduction) and may not be practically significant, especially given the doubled computational cost.', 'No comparison to stronger or more sophisticated baselines (e.g., attention, hierarchical, or multi-scale architectures used in diffusion models).', 'No discussion of negative results, failure cases, or broader applicability.', 'The work does not advance the state of the art in generative modeling in any substantive way.']\", \"Originality\": 2, \"Quality\": 2, \"Clarity\": 3, \"Significance\": 1, \"Decision\": \"Reject\"}",
        "novel": false
    },
    {
        "Name": "learned_geometric_guidance",
        "Title": "Learned Geometric Guidance for Diffusion Models",
        "Problem Statement": "Diffusion models lack mechanisms to automatically discover and exploit the geometric structure of low-dimensional data during generation.",
        "Motivation": "Learning geometric properties during training can provide more flexible and automatic structural guidance than hand-designed constraints, while maintaining interpretability and control.",
        "Proposed Method": "1) Augment the denoiser to predict geometric features (e.g., manifold tangents, curvature) 2) Add geometric consistency losses during training 3) Use predicted features to guide sampling via projection/interpolation.",
        "Experiment": "1) Modify MLPDenoiser to output geometric features 2) Add geometric loss terms to training 3) Compare to baseline on: a) Sample quality metrics b) Ability to control generation via geometric properties c) Visualization of learned features 4) Analyze guidance strength scheduling.",
        "Interestingness": 9,
        "Feasibility": 7,
        "Novelty": 8,
        "lit_review_results": "title: Denoising diffusion probabilistic models are optimally adaptive to   unknown low dimensionality\nabstract: The denoising diffusion probabilistic model (DDPM) has emerged as a mainstream generative model in generative AI. While sharp convergence guarantees have been established for the DDPM, the iteration complexity is, in general, proportional to the ambient data dimension, resulting in overly conservative theory that fails to explain its practical efficiency. This has motivated the recent work Li and Yan (2024a) to investigate how the DDPM can achieve sampling speed-ups through automatic exploitation of intrinsic low dimensionality of data. We strengthen this line of work by demonstrating, in some sense, optimal adaptivity to unknown low dimensionality. For a broad class of data distributions with intrinsic dimension $k$, we prove that the iteration complexity of the DDPM scales nearly linearly with $k$, which is optimal when using KL divergence to measure distributional discrepancy. Notably, our work is closely aligned with the independent concurrent work Potaptchik et al. (2024) -- posted two weeks prior to ours -- in establishing nearly linear-$k$ convergence guarantees for the DDPM.\n\ntitle: Denoising Diffusion Probabilistic Models in Six Simple Steps\nabstract: Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but they have a high barrier-to-entry as they require background knowledge of stochastic differential equations and probability flow. In this note, we distill down the formulation of the DDPM into six simple steps each of which comes with a clear rationale. We assume that the reader is familiar with fundamental topics in machine learning including basic probabilistic modelling, Gaussian distributions, maximum likelihood estimation, and deep learning.\n\ntitle: Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion   Models\nabstract: This paper investigates score-based diffusion models when the underlying target distribution is concentrated on or near low-dimensional manifolds within the higher-dimensional space in which they formally reside, a common characteristic of natural image distributions. Despite previous efforts to understand the data generation process of diffusion models, existing theoretical support remains highly suboptimal in the presence of low-dimensional structure, which we strengthen in this paper. For the popular Denoising Diffusion Probabilistic Model (DDPM), we find that the dependency of the error incurred within each denoising step on the ambient dimension $d$ is in general unavoidable. We further identify a unique design of coefficients that yields a converges rate at the order of $O(k^{2}/\\sqrt{T})$ (up to log factors), where $k$ is the intrinsic dimension of the target distribution and $T$ is the number of steps. This represents the first theoretical demonstration that the DDPM sampler can adapt to unknown low-dimensional structures in the target distribution, highlighting the critical importance of coefficient design. All of this is achieved by a novel set of analysis tools that characterize the algorithmic dynamics in a more deterministic manner.\n\ntitle: Structured Denoising Diffusion Models in Discrete State-Spaces\nabstract: Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.\n\ntitle: Improved Denoising Diffusion Probabilistic Models\nabstract: Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion\n\n",
        "paper": "title: DUALSCALE DIFFUSION: ADAPTIVE FEATURE BAL- ANCING FOR LOW-DIMENSIONAL GENERATIVE MOD- ELS\nabstract: This paper introduces an adaptive dual-scale denoising approach for low- dimensional diffusion models, addressing the challenge of balancing global struc- ture and local detail in generated samples. While diffusion models have shown re- markable success in high-dimensional spaces, their application to low-dimensional data remains crucial for understanding fundamental model behaviors and address- ing real-world applications with inherently low-dimensional data. However, in these spaces, traditional models often struggle to simultaneously capture both macro-level patterns and fine-grained features, leading to suboptimal sample qual- ity. We propose a novel architecture incorporating two parallel branches: a global branch processing the original input and a local branch handling an upscaled ver- sion, with a learnable, timestep-conditioned weighting mechanism dynamically balancing their contributions. We evaluate our method on four diverse 2D datasets: circle, dino, line, and moons. Our results demonstrate significant improvements in sample quality, with KL divergence reductions of up to 12.8% compared to the baseline model. The adaptive weighting successfully adjusts the focus be- tween global and local features across different datasets and denoising stages, as evidenced by our weight evolution analysis. This work not only enhances low-dimensional diffusion models but also provides insights that could inform improvements in higher-dimensional domains, opening new avenues for advancing generative modeling across various applications.",
        "paper_review_results": "{\"paper_id\": \"DualScale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models\", \"Summary\": \"This paper proposes an adaptive dual-scale denoising architecture for low-dimensional (2D) diffusion models. The model consists of two parallel MLP branches\\u2014one processing the original (global) features and the other an upscaled (local) version\\u2014whose outputs are combined via a learnable, timestep-conditioned weighting mechanism. The approach is evaluated on four synthetic 2D datasets, showing modest improvements in KL divergence and visual sample quality compared to a single-branch baseline. Ablation studies and analysis of the learned weights are provided.\", \"Questions\": \"['Can the authors provide theoretical or empirical motivation for why dual-scale adaptive weighting should help in diffusion models, especially in low dimensions?', 'Why is the evaluation limited to synthetic 2D datasets? Have the authors attempted to apply the method to higher-dimensional or real-world data?', 'How does the method compare to a single-branch MLP of equivalent or higher capacity, or to other adaptive feature fusion techniques (e.g., attention, gating, hierarchical models)?', 'Is the upscaling operation critical? What happens if the local branch simply reprocesses the original input, or if the upscaled dimension is varied?', 'Are the reported KL improvements statistically significant and consistent across different random seeds?', 'Can the authors clarify the precise upscaling operation and justify the choice of projection dimension?']\", \"Limitations\": \"['The method is only evaluated on simple 2D synthetic datasets; its applicability to real-world or higher-dimensional data is untested.', 'The computational overhead is significant relative to the modest performance improvement.', 'No theoretical understanding or insight is offered for when dual-scale adaptive weighting is necessary or effective.', 'The adaptive weighting mechanism may overfit to the specific datasets and may not generalize.', 'No discussion of potential negative societal impacts or misuse, though this is less relevant for toy generative modeling.']\", \"Ethical Concerns\": false, \"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Overall\": 2, \"Confidence\": 4, \"Strengths\": \"['The paper is clearly written and well-organized, with explicit architectural descriptions.', 'The proposed adaptive weighting mechanism is simple, interpretable, and easy to implement.', 'Experiments are reproducible, with code availability and detailed experimental settings.', 'Ablation studies and analysis of the weighting mechanism are included.']\", \"Weaknesses\": \"['Technical novelty is minimal: the architecture is a straightforward combination of two MLP branches with learnable weighting, an established idea in deep learning.', 'No theoretical justification or principled motivation is provided for why adaptive dual-scale processing is beneficial in diffusion models.', 'Empirical evaluation is limited to trivial synthetic 2D datasets; there is no evidence the method generalizes to real or higher-dimensional data.', 'Reported improvements are modest (2-13% KL reduction) and may not be practically significant, especially given the doubled computational cost.', 'No comparison to stronger or more sophisticated baselines (e.g., attention, hierarchical, or multi-scale architectures used in diffusion models).', 'No discussion of negative results, failure cases, or broader applicability.', 'The work does not advance the state of the art in generative modeling in any substantive way.']\", \"Originality\": 2, \"Quality\": 2, \"Clarity\": 3, \"Significance\": 1, \"Decision\": \"Reject\"}",
        "novel": true
    },
    {
        "Name": "geometric_manifold_guidance",
        "Title": "Manifold-Aware Diffusion via Learned Geometric Guidance",
        "Problem Statement": "Current diffusion models treat all dimensions equally during generation, ignoring the underlying geometric structure of low-dimensional data manifolds.",
        "Motivation": "Explicitly modeling and utilizing manifold geometry could improve sample quality and enable more intuitive control over generation, particularly for structured low-dimensional data.",
        "Proposed Method": "Augment the denoiser to predict local tangent spaces, then use these predictions to guide sampling via geometric projections that respect the learned manifold structure.",
        "Experiment": "1) Modify MLPDenoiser to output tangent basis vectors 2) Add tangent space consistency loss during training 3) Implement manifold projection in NoiseScheduler.step() 4) Evaluate via: a) Sample quality metrics (KL) b) Tangent space alignment error c) Precision of geometric control",
        "Interestingness": 9,
        "Feasibility": 8,
        "Novelty": 8,
        "lit_review_results": "title: Denoising diffusion probabilistic models are optimally adaptive to   unknown low dimensionality\nabstract: The denoising diffusion probabilistic model (DDPM) has emerged as a mainstream generative model in generative AI. While sharp convergence guarantees have been established for the DDPM, the iteration complexity is, in general, proportional to the ambient data dimension, resulting in overly conservative theory that fails to explain its practical efficiency. This has motivated the recent work Li and Yan (2024a) to investigate how the DDPM can achieve sampling speed-ups through automatic exploitation of intrinsic low dimensionality of data. We strengthen this line of work by demonstrating, in some sense, optimal adaptivity to unknown low dimensionality. For a broad class of data distributions with intrinsic dimension $k$, we prove that the iteration complexity of the DDPM scales nearly linearly with $k$, which is optimal when using KL divergence to measure distributional discrepancy. Notably, our work is closely aligned with the independent concurrent work Potaptchik et al. (2024) -- posted two weeks prior to ours -- in establishing nearly linear-$k$ convergence guarantees for the DDPM.\n\ntitle: Denoising Diffusion Probabilistic Models in Six Simple Steps\nabstract: Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but they have a high barrier-to-entry as they require background knowledge of stochastic differential equations and probability flow. In this note, we distill down the formulation of the DDPM into six simple steps each of which comes with a clear rationale. We assume that the reader is familiar with fundamental topics in machine learning including basic probabilistic modelling, Gaussian distributions, maximum likelihood estimation, and deep learning.\n\ntitle: Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion   Models\nabstract: This paper investigates score-based diffusion models when the underlying target distribution is concentrated on or near low-dimensional manifolds within the higher-dimensional space in which they formally reside, a common characteristic of natural image distributions. Despite previous efforts to understand the data generation process of diffusion models, existing theoretical support remains highly suboptimal in the presence of low-dimensional structure, which we strengthen in this paper. For the popular Denoising Diffusion Probabilistic Model (DDPM), we find that the dependency of the error incurred within each denoising step on the ambient dimension $d$ is in general unavoidable. We further identify a unique design of coefficients that yields a converges rate at the order of $O(k^{2}/\\sqrt{T})$ (up to log factors), where $k$ is the intrinsic dimension of the target distribution and $T$ is the number of steps. This represents the first theoretical demonstration that the DDPM sampler can adapt to unknown low-dimensional structures in the target distribution, highlighting the critical importance of coefficient design. All of this is achieved by a novel set of analysis tools that characterize the algorithmic dynamics in a more deterministic manner.\n\ntitle: Structured Denoising Diffusion Models in Discrete State-Spaces\nabstract: Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.\n\ntitle: Improved Denoising Diffusion Probabilistic Models\nabstract: Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion\n\n",
        "paper": "title: DUALSCALE DIFFUSION: ADAPTIVE FEATURE BAL- ANCING FOR LOW-DIMENSIONAL GENERATIVE MOD- ELS\nabstract: This paper introduces an adaptive dual-scale denoising approach for low- dimensional diffusion models, addressing the challenge of balancing global struc- ture and local detail in generated samples. While diffusion models have shown re- markable success in high-dimensional spaces, their application to low-dimensional data remains crucial for understanding fundamental model behaviors and address- ing real-world applications with inherently low-dimensional data. However, in these spaces, traditional models often struggle to simultaneously capture both macro-level patterns and fine-grained features, leading to suboptimal sample qual- ity. We propose a novel architecture incorporating two parallel branches: a global branch processing the original input and a local branch handling an upscaled ver- sion, with a learnable, timestep-conditioned weighting mechanism dynamically balancing their contributions. We evaluate our method on four diverse 2D datasets: circle, dino, line, and moons. Our results demonstrate significant improvements in sample quality, with KL divergence reductions of up to 12.8% compared to the baseline model. The adaptive weighting successfully adjusts the focus be- tween global and local features across different datasets and denoising stages, as evidenced by our weight evolution analysis. This work not only enhances low-dimensional diffusion models but also provides insights that could inform improvements in higher-dimensional domains, opening new avenues for advancing generative modeling across various applications.",
        "paper_review_results": "{\"paper_id\": \"DualScale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models\", \"Summary\": \"This paper proposes an adaptive dual-scale denoising architecture for low-dimensional (2D) diffusion models. The model consists of two parallel MLP branches\\u2014one processing the original (global) features and the other an upscaled (local) version\\u2014whose outputs are combined via a learnable, timestep-conditioned weighting mechanism. The approach is evaluated on four synthetic 2D datasets, showing modest improvements in KL divergence and visual sample quality compared to a single-branch baseline. Ablation studies and analysis of the learned weights are provided.\", \"Questions\": \"['Can the authors provide theoretical or empirical motivation for why dual-scale adaptive weighting should help in diffusion models, especially in low dimensions?', 'Why is the evaluation limited to synthetic 2D datasets? Have the authors attempted to apply the method to higher-dimensional or real-world data?', 'How does the method compare to a single-branch MLP of equivalent or higher capacity, or to other adaptive feature fusion techniques (e.g., attention, gating, hierarchical models)?', 'Is the upscaling operation critical? What happens if the local branch simply reprocesses the original input, or if the upscaled dimension is varied?', 'Are the reported KL improvements statistically significant and consistent across different random seeds?', 'Can the authors clarify the precise upscaling operation and justify the choice of projection dimension?']\", \"Limitations\": \"['The method is only evaluated on simple 2D synthetic datasets; its applicability to real-world or higher-dimensional data is untested.', 'The computational overhead is significant relative to the modest performance improvement.', 'No theoretical understanding or insight is offered for when dual-scale adaptive weighting is necessary or effective.', 'The adaptive weighting mechanism may overfit to the specific datasets and may not generalize.', 'No discussion of potential negative societal impacts or misuse, though this is less relevant for toy generative modeling.']\", \"Ethical Concerns\": false, \"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Overall\": 2, \"Confidence\": 4, \"Strengths\": \"['The paper is clearly written and well-organized, with explicit architectural descriptions.', 'The proposed adaptive weighting mechanism is simple, interpretable, and easy to implement.', 'Experiments are reproducible, with code availability and detailed experimental settings.', 'Ablation studies and analysis of the weighting mechanism are included.']\", \"Weaknesses\": \"['Technical novelty is minimal: the architecture is a straightforward combination of two MLP branches with learnable weighting, an established idea in deep learning.', 'No theoretical justification or principled motivation is provided for why adaptive dual-scale processing is beneficial in diffusion models.', 'Empirical evaluation is limited to trivial synthetic 2D datasets; there is no evidence the method generalizes to real or higher-dimensional data.', 'Reported improvements are modest (2-13% KL reduction) and may not be practically significant, especially given the doubled computational cost.', 'No comparison to stronger or more sophisticated baselines (e.g., attention, hierarchical, or multi-scale architectures used in diffusion models).', 'No discussion of negative results, failure cases, or broader applicability.', 'The work does not advance the state of the art in generative modeling in any substantive way.']\", \"Originality\": 2, \"Quality\": 2, \"Clarity\": 3, \"Significance\": 1, \"Decision\": \"Reject\"}",
        "novel": true
    },
    {
        "Name": "density_aware_diffusion_guidance",
        "Title": "Density-Aware Diffusion Guidance for Balanced Sampling",
        "Problem Statement": "Diffusion models often sample unevenly from probability density regions, over-sampling high-density areas and under-sampling low-density regions.",
        "Motivation": "More balanced sampling across all density regions improves both sample quality and coverage of the full data distribution, especially important for downstream applications.",
        "Proposed Method": "Augment the denoising process with density estimation to guide sampling toward under-explored regions: 1) Add a lightweight density estimation head to the denoiser, 2) Implement density-aware guidance during sampling that balances exploration/exploitation, 3) Maintain the original denoising as a base process.",
        "Experiment": "1) Modify MLPDenoiser to output density estimates 2) Add density guidance to NoiseScheduler.step() 3) Evaluate on all datasets using: a) Coverage metrics (e.g. KL divergence) b) Density distribution matching c) Visual sample quality 4) Compare density-aware vs original sampling",
        "Interestingness": 9,
        "Feasibility": 9,
        "Novelty": 7,
        "lit_review_results": "title: Denoising diffusion probabilistic models are optimally adaptive to   unknown low dimensionality\nabstract: The denoising diffusion probabilistic model (DDPM) has emerged as a mainstream generative model in generative AI. While sharp convergence guarantees have been established for the DDPM, the iteration complexity is, in general, proportional to the ambient data dimension, resulting in overly conservative theory that fails to explain its practical efficiency. This has motivated the recent work Li and Yan (2024a) to investigate how the DDPM can achieve sampling speed-ups through automatic exploitation of intrinsic low dimensionality of data. We strengthen this line of work by demonstrating, in some sense, optimal adaptivity to unknown low dimensionality. For a broad class of data distributions with intrinsic dimension $k$, we prove that the iteration complexity of the DDPM scales nearly linearly with $k$, which is optimal when using KL divergence to measure distributional discrepancy. Notably, our work is closely aligned with the independent concurrent work Potaptchik et al. (2024) -- posted two weeks prior to ours -- in establishing nearly linear-$k$ convergence guarantees for the DDPM.\n\ntitle: Denoising Diffusion Probabilistic Models in Six Simple Steps\nabstract: Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but they have a high barrier-to-entry as they require background knowledge of stochastic differential equations and probability flow. In this note, we distill down the formulation of the DDPM into six simple steps each of which comes with a clear rationale. We assume that the reader is familiar with fundamental topics in machine learning including basic probabilistic modelling, Gaussian distributions, maximum likelihood estimation, and deep learning.\n\ntitle: Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion   Models\nabstract: This paper investigates score-based diffusion models when the underlying target distribution is concentrated on or near low-dimensional manifolds within the higher-dimensional space in which they formally reside, a common characteristic of natural image distributions. Despite previous efforts to understand the data generation process of diffusion models, existing theoretical support remains highly suboptimal in the presence of low-dimensional structure, which we strengthen in this paper. For the popular Denoising Diffusion Probabilistic Model (DDPM), we find that the dependency of the error incurred within each denoising step on the ambient dimension $d$ is in general unavoidable. We further identify a unique design of coefficients that yields a converges rate at the order of $O(k^{2}/\\sqrt{T})$ (up to log factors), where $k$ is the intrinsic dimension of the target distribution and $T$ is the number of steps. This represents the first theoretical demonstration that the DDPM sampler can adapt to unknown low-dimensional structures in the target distribution, highlighting the critical importance of coefficient design. All of this is achieved by a novel set of analysis tools that characterize the algorithmic dynamics in a more deterministic manner.\n\ntitle: Structured Denoising Diffusion Models in Discrete State-Spaces\nabstract: Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.\n\ntitle: Improved Denoising Diffusion Probabilistic Models\nabstract: Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion\n\n",
        "paper": "title: DUALSCALE DIFFUSION: ADAPTIVE FEATURE BAL- ANCING FOR LOW-DIMENSIONAL GENERATIVE MOD- ELS\nabstract: This paper introduces an adaptive dual-scale denoising approach for low- dimensional diffusion models, addressing the challenge of balancing global struc- ture and local detail in generated samples. While diffusion models have shown re- markable success in high-dimensional spaces, their application to low-dimensional data remains crucial for understanding fundamental model behaviors and address- ing real-world applications with inherently low-dimensional data. However, in these spaces, traditional models often struggle to simultaneously capture both macro-level patterns and fine-grained features, leading to suboptimal sample qual- ity. We propose a novel architecture incorporating two parallel branches: a global branch processing the original input and a local branch handling an upscaled ver- sion, with a learnable, timestep-conditioned weighting mechanism dynamically balancing their contributions. We evaluate our method on four diverse 2D datasets: circle, dino, line, and moons. Our results demonstrate significant improvements in sample quality, with KL divergence reductions of up to 12.8% compared to the baseline model. The adaptive weighting successfully adjusts the focus be- tween global and local features across different datasets and denoising stages, as evidenced by our weight evolution analysis. This work not only enhances low-dimensional diffusion models but also provides insights that could inform improvements in higher-dimensional domains, opening new avenues for advancing generative modeling across various applications.",
        "paper_review_results": "{\"paper_id\": \"DualScale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models\", \"Summary\": \"This paper proposes an adaptive dual-scale denoising architecture for low-dimensional (2D) diffusion models. The model consists of two parallel MLP branches\\u2014one processing the original (global) features and the other an upscaled (local) version\\u2014whose outputs are combined via a learnable, timestep-conditioned weighting mechanism. The approach is evaluated on four synthetic 2D datasets, showing modest improvements in KL divergence and visual sample quality compared to a single-branch baseline. Ablation studies and analysis of the learned weights are provided.\", \"Questions\": \"['Can the authors provide theoretical or empirical motivation for why dual-scale adaptive weighting should help in diffusion models, especially in low dimensions?', 'Why is the evaluation limited to synthetic 2D datasets? Have the authors attempted to apply the method to higher-dimensional or real-world data?', 'How does the method compare to a single-branch MLP of equivalent or higher capacity, or to other adaptive feature fusion techniques (e.g., attention, gating, hierarchical models)?', 'Is the upscaling operation critical? What happens if the local branch simply reprocesses the original input, or if the upscaled dimension is varied?', 'Are the reported KL improvements statistically significant and consistent across different random seeds?', 'Can the authors clarify the precise upscaling operation and justify the choice of projection dimension?']\", \"Limitations\": \"['The method is only evaluated on simple 2D synthetic datasets; its applicability to real-world or higher-dimensional data is untested.', 'The computational overhead is significant relative to the modest performance improvement.', 'No theoretical understanding or insight is offered for when dual-scale adaptive weighting is necessary or effective.', 'The adaptive weighting mechanism may overfit to the specific datasets and may not generalize.', 'No discussion of potential negative societal impacts or misuse, though this is less relevant for toy generative modeling.']\", \"Ethical Concerns\": false, \"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Overall\": 2, \"Confidence\": 4, \"Strengths\": \"['The paper is clearly written and well-organized, with explicit architectural descriptions.', 'The proposed adaptive weighting mechanism is simple, interpretable, and easy to implement.', 'Experiments are reproducible, with code availability and detailed experimental settings.', 'Ablation studies and analysis of the weighting mechanism are included.']\", \"Weaknesses\": \"['Technical novelty is minimal: the architecture is a straightforward combination of two MLP branches with learnable weighting, an established idea in deep learning.', 'No theoretical justification or principled motivation is provided for why adaptive dual-scale processing is beneficial in diffusion models.', 'Empirical evaluation is limited to trivial synthetic 2D datasets; there is no evidence the method generalizes to real or higher-dimensional data.', 'Reported improvements are modest (2-13% KL reduction) and may not be practically significant, especially given the doubled computational cost.', 'No comparison to stronger or more sophisticated baselines (e.g., attention, hierarchical, or multi-scale architectures used in diffusion models).', 'No discussion of negative results, failure cases, or broader applicability.', 'The work does not advance the state of the art in generative modeling in any substantive way.']\", \"Originality\": 2, \"Quality\": 2, \"Clarity\": 3, \"Significance\": 1, \"Decision\": \"Reject\"}",
        "novel": true
    },
    {
        "Name": "hierarchical_control_diffusion",
        "Title": "Hierarchical Control of Diffusion Models via Timestep-Conditioned Constraints",
        "Problem Statement": "Current diffusion models lack intuitive control mechanisms that align with their natural hierarchical denoising process, making it difficult to guide generation at different scales.",
        "Motivation": "The diffusion process inherently operates at multiple scales - early steps determine global structure while later steps refine details. We should leverage this property to enable more intuitive control over generation.",
        "Proposed Method": "Implement a constraint system that operates at different timestep ranges, allowing users to specify desired properties at different scales (global shape vs local details). Constraints are applied via gradient guidance but weighted differently based on their assigned timestep range.",
        "Experiment": "1) Modify NoiseScheduler.step() to accept and apply scale-specific constraints 2) Define constraint functions for each dataset (e.g., quadrant constraints for global, cluster membership for local) 3) Evaluate: a) Constraint satisfaction at different scales b) Sample quality metrics c) Comparison to non-hierarchical control methods",
        "Interestingness": 9,
        "Feasibility": 8,
        "Novelty": 8,
        "lit_review_results": "title: Denoising diffusion probabilistic models are optimally adaptive to   unknown low dimensionality\nabstract: The denoising diffusion probabilistic model (DDPM) has emerged as a mainstream generative model in generative AI. While sharp convergence guarantees have been established for the DDPM, the iteration complexity is, in general, proportional to the ambient data dimension, resulting in overly conservative theory that fails to explain its practical efficiency. This has motivated the recent work Li and Yan (2024a) to investigate how the DDPM can achieve sampling speed-ups through automatic exploitation of intrinsic low dimensionality of data. We strengthen this line of work by demonstrating, in some sense, optimal adaptivity to unknown low dimensionality. For a broad class of data distributions with intrinsic dimension $k$, we prove that the iteration complexity of the DDPM scales nearly linearly with $k$, which is optimal when using KL divergence to measure distributional discrepancy. Notably, our work is closely aligned with the independent concurrent work Potaptchik et al. (2024) -- posted two weeks prior to ours -- in establishing nearly linear-$k$ convergence guarantees for the DDPM.\n\ntitle: Denoising Diffusion Probabilistic Models in Six Simple Steps\nabstract: Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but they have a high barrier-to-entry as they require background knowledge of stochastic differential equations and probability flow. In this note, we distill down the formulation of the DDPM into six simple steps each of which comes with a clear rationale. We assume that the reader is familiar with fundamental topics in machine learning including basic probabilistic modelling, Gaussian distributions, maximum likelihood estimation, and deep learning.\n\ntitle: Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion   Models\nabstract: This paper investigates score-based diffusion models when the underlying target distribution is concentrated on or near low-dimensional manifolds within the higher-dimensional space in which they formally reside, a common characteristic of natural image distributions. Despite previous efforts to understand the data generation process of diffusion models, existing theoretical support remains highly suboptimal in the presence of low-dimensional structure, which we strengthen in this paper. For the popular Denoising Diffusion Probabilistic Model (DDPM), we find that the dependency of the error incurred within each denoising step on the ambient dimension $d$ is in general unavoidable. We further identify a unique design of coefficients that yields a converges rate at the order of $O(k^{2}/\\sqrt{T})$ (up to log factors), where $k$ is the intrinsic dimension of the target distribution and $T$ is the number of steps. This represents the first theoretical demonstration that the DDPM sampler can adapt to unknown low-dimensional structures in the target distribution, highlighting the critical importance of coefficient design. All of this is achieved by a novel set of analysis tools that characterize the algorithmic dynamics in a more deterministic manner.\n\ntitle: Structured Denoising Diffusion Models in Discrete State-Spaces\nabstract: Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.\n\ntitle: Improved Denoising Diffusion Probabilistic Models\nabstract: Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion\n\n",
        "paper": "title: DUALSCALE DIFFUSION: ADAPTIVE FEATURE BAL- ANCING FOR LOW-DIMENSIONAL GENERATIVE MOD- ELS\nabstract: This paper introduces an adaptive dual-scale denoising approach for low- dimensional diffusion models, addressing the challenge of balancing global struc- ture and local detail in generated samples. While diffusion models have shown re- markable success in high-dimensional spaces, their application to low-dimensional data remains crucial for understanding fundamental model behaviors and address- ing real-world applications with inherently low-dimensional data. However, in these spaces, traditional models often struggle to simultaneously capture both macro-level patterns and fine-grained features, leading to suboptimal sample qual- ity. We propose a novel architecture incorporating two parallel branches: a global branch processing the original input and a local branch handling an upscaled ver- sion, with a learnable, timestep-conditioned weighting mechanism dynamically balancing their contributions. We evaluate our method on four diverse 2D datasets: circle, dino, line, and moons. Our results demonstrate significant improvements in sample quality, with KL divergence reductions of up to 12.8% compared to the baseline model. The adaptive weighting successfully adjusts the focus be- tween global and local features across different datasets and denoising stages, as evidenced by our weight evolution analysis. This work not only enhances low-dimensional diffusion models but also provides insights that could inform improvements in higher-dimensional domains, opening new avenues for advancing generative modeling across various applications.",
        "paper_review_results": "{\"paper_id\": \"DualScale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models\", \"Summary\": \"This paper proposes an adaptive dual-scale denoising architecture for low-dimensional (2D) diffusion models. The model consists of two parallel MLP branches\\u2014one processing the original (global) features and the other an upscaled (local) version\\u2014whose outputs are combined via a learnable, timestep-conditioned weighting mechanism. The approach is evaluated on four synthetic 2D datasets, showing modest improvements in KL divergence and visual sample quality compared to a single-branch baseline. Ablation studies and analysis of the learned weights are provided.\", \"Questions\": \"['Can the authors provide theoretical or empirical motivation for why dual-scale adaptive weighting should help in diffusion models, especially in low dimensions?', 'Why is the evaluation limited to synthetic 2D datasets? Have the authors attempted to apply the method to higher-dimensional or real-world data?', 'How does the method compare to a single-branch MLP of equivalent or higher capacity, or to other adaptive feature fusion techniques (e.g., attention, gating, hierarchical models)?', 'Is the upscaling operation critical? What happens if the local branch simply reprocesses the original input, or if the upscaled dimension is varied?', 'Are the reported KL improvements statistically significant and consistent across different random seeds?', 'Can the authors clarify the precise upscaling operation and justify the choice of projection dimension?']\", \"Limitations\": \"['The method is only evaluated on simple 2D synthetic datasets; its applicability to real-world or higher-dimensional data is untested.', 'The computational overhead is significant relative to the modest performance improvement.', 'No theoretical understanding or insight is offered for when dual-scale adaptive weighting is necessary or effective.', 'The adaptive weighting mechanism may overfit to the specific datasets and may not generalize.', 'No discussion of potential negative societal impacts or misuse, though this is less relevant for toy generative modeling.']\", \"Ethical Concerns\": false, \"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Overall\": 2, \"Confidence\": 4, \"Strengths\": \"['The paper is clearly written and well-organized, with explicit architectural descriptions.', 'The proposed adaptive weighting mechanism is simple, interpretable, and easy to implement.', 'Experiments are reproducible, with code availability and detailed experimental settings.', 'Ablation studies and analysis of the weighting mechanism are included.']\", \"Weaknesses\": \"['Technical novelty is minimal: the architecture is a straightforward combination of two MLP branches with learnable weighting, an established idea in deep learning.', 'No theoretical justification or principled motivation is provided for why adaptive dual-scale processing is beneficial in diffusion models.', 'Empirical evaluation is limited to trivial synthetic 2D datasets; there is no evidence the method generalizes to real or higher-dimensional data.', 'Reported improvements are modest (2-13% KL reduction) and may not be practically significant, especially given the doubled computational cost.', 'No comparison to stronger or more sophisticated baselines (e.g., attention, hierarchical, or multi-scale architectures used in diffusion models).', 'No discussion of negative results, failure cases, or broader applicability.', 'The work does not advance the state of the art in generative modeling in any substantive way.']\", \"Originality\": 2, \"Quality\": 2, \"Clarity\": 3, \"Significance\": 1, \"Decision\": \"Reject\"}",
        "novel": true
    },
    {
        "Name": "manifold_guided_diffusion",
        "Title": "Manifold-Aware Diffusion via Tangent Space Prediction",
        "Problem Statement": "Current diffusion sampling processes treat all directions equally, ignoring the intrinsic low-dimensional structure of data manifolds that could guide more efficient generation.",
        "Motivation": "Learning and utilizing local manifold geometry during sampling could improve both sample quality and generation efficiency while maintaining the simplicity of the base diffusion approach.",
        "Proposed Method": "1) Extend the denoiser to predict local tangent space directions 2) Optionally project sampling steps onto predicted tangent spaces 3) Compare guided vs unguided sampling modes.",
        "Experiment": "1) Add tangent space prediction head to MLPDenoiser 2) Modify NoiseScheduler.step() with optional tangent projection 3) Evaluate on all datasets using: a) Sample quality (KL) b) Sample diversity (coverage metrics) c) Sampling efficiency (steps needed).",
        "Interestingness": 9,
        "Feasibility": 9,
        "Novelty": 8,
        "lit_review_results": "title: Denoising diffusion probabilistic models are optimally adaptive to   unknown low dimensionality\nabstract: The denoising diffusion probabilistic model (DDPM) has emerged as a mainstream generative model in generative AI. While sharp convergence guarantees have been established for the DDPM, the iteration complexity is, in general, proportional to the ambient data dimension, resulting in overly conservative theory that fails to explain its practical efficiency. This has motivated the recent work Li and Yan (2024a) to investigate how the DDPM can achieve sampling speed-ups through automatic exploitation of intrinsic low dimensionality of data. We strengthen this line of work by demonstrating, in some sense, optimal adaptivity to unknown low dimensionality. For a broad class of data distributions with intrinsic dimension $k$, we prove that the iteration complexity of the DDPM scales nearly linearly with $k$, which is optimal when using KL divergence to measure distributional discrepancy. Notably, our work is closely aligned with the independent concurrent work Potaptchik et al. (2024) -- posted two weeks prior to ours -- in establishing nearly linear-$k$ convergence guarantees for the DDPM.\n\ntitle: Denoising Diffusion Probabilistic Models in Six Simple Steps\nabstract: Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but they have a high barrier-to-entry as they require background knowledge of stochastic differential equations and probability flow. In this note, we distill down the formulation of the DDPM into six simple steps each of which comes with a clear rationale. We assume that the reader is familiar with fundamental topics in machine learning including basic probabilistic modelling, Gaussian distributions, maximum likelihood estimation, and deep learning.\n\ntitle: Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion   Models\nabstract: This paper investigates score-based diffusion models when the underlying target distribution is concentrated on or near low-dimensional manifolds within the higher-dimensional space in which they formally reside, a common characteristic of natural image distributions. Despite previous efforts to understand the data generation process of diffusion models, existing theoretical support remains highly suboptimal in the presence of low-dimensional structure, which we strengthen in this paper. For the popular Denoising Diffusion Probabilistic Model (DDPM), we find that the dependency of the error incurred within each denoising step on the ambient dimension $d$ is in general unavoidable. We further identify a unique design of coefficients that yields a converges rate at the order of $O(k^{2}/\\sqrt{T})$ (up to log factors), where $k$ is the intrinsic dimension of the target distribution and $T$ is the number of steps. This represents the first theoretical demonstration that the DDPM sampler can adapt to unknown low-dimensional structures in the target distribution, highlighting the critical importance of coefficient design. All of this is achieved by a novel set of analysis tools that characterize the algorithmic dynamics in a more deterministic manner.\n\ntitle: Structured Denoising Diffusion Models in Discrete State-Spaces\nabstract: Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.\n\ntitle: Improved Denoising Diffusion Probabilistic Models\nabstract: Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion\n\n",
        "paper": "title: DUALSCALE DIFFUSION: ADAPTIVE FEATURE BAL- ANCING FOR LOW-DIMENSIONAL GENERATIVE MOD- ELS\nabstract: This paper introduces an adaptive dual-scale denoising approach for low- dimensional diffusion models, addressing the challenge of balancing global struc- ture and local detail in generated samples. While diffusion models have shown re- markable success in high-dimensional spaces, their application to low-dimensional data remains crucial for understanding fundamental model behaviors and address- ing real-world applications with inherently low-dimensional data. However, in these spaces, traditional models often struggle to simultaneously capture both macro-level patterns and fine-grained features, leading to suboptimal sample qual- ity. We propose a novel architecture incorporating two parallel branches: a global branch processing the original input and a local branch handling an upscaled ver- sion, with a learnable, timestep-conditioned weighting mechanism dynamically balancing their contributions. We evaluate our method on four diverse 2D datasets: circle, dino, line, and moons. Our results demonstrate significant improvements in sample quality, with KL divergence reductions of up to 12.8% compared to the baseline model. The adaptive weighting successfully adjusts the focus be- tween global and local features across different datasets and denoising stages, as evidenced by our weight evolution analysis. This work not only enhances low-dimensional diffusion models but also provides insights that could inform improvements in higher-dimensional domains, opening new avenues for advancing generative modeling across various applications.",
        "paper_review_results": "{\"paper_id\": \"DualScale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models\", \"Summary\": \"This paper proposes an adaptive dual-scale denoising architecture for low-dimensional (2D) diffusion models. The model consists of two parallel MLP branches\\u2014one processing the original (global) features and the other an upscaled (local) version\\u2014whose outputs are combined via a learnable, timestep-conditioned weighting mechanism. The approach is evaluated on four synthetic 2D datasets, showing modest improvements in KL divergence and visual sample quality compared to a single-branch baseline. Ablation studies and analysis of the learned weights are provided.\", \"Questions\": \"['Can the authors provide theoretical or empirical motivation for why dual-scale adaptive weighting should help in diffusion models, especially in low dimensions?', 'Why is the evaluation limited to synthetic 2D datasets? Have the authors attempted to apply the method to higher-dimensional or real-world data?', 'How does the method compare to a single-branch MLP of equivalent or higher capacity, or to other adaptive feature fusion techniques (e.g., attention, gating, hierarchical models)?', 'Is the upscaling operation critical? What happens if the local branch simply reprocesses the original input, or if the upscaled dimension is varied?', 'Are the reported KL improvements statistically significant and consistent across different random seeds?', 'Can the authors clarify the precise upscaling operation and justify the choice of projection dimension?']\", \"Limitations\": \"['The method is only evaluated on simple 2D synthetic datasets; its applicability to real-world or higher-dimensional data is untested.', 'The computational overhead is significant relative to the modest performance improvement.', 'No theoretical understanding or insight is offered for when dual-scale adaptive weighting is necessary or effective.', 'The adaptive weighting mechanism may overfit to the specific datasets and may not generalize.', 'No discussion of potential negative societal impacts or misuse, though this is less relevant for toy generative modeling.']\", \"Ethical Concerns\": false, \"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Overall\": 2, \"Confidence\": 4, \"Strengths\": \"['The paper is clearly written and well-organized, with explicit architectural descriptions.', 'The proposed adaptive weighting mechanism is simple, interpretable, and easy to implement.', 'Experiments are reproducible, with code availability and detailed experimental settings.', 'Ablation studies and analysis of the weighting mechanism are included.']\", \"Weaknesses\": \"['Technical novelty is minimal: the architecture is a straightforward combination of two MLP branches with learnable weighting, an established idea in deep learning.', 'No theoretical justification or principled motivation is provided for why adaptive dual-scale processing is beneficial in diffusion models.', 'Empirical evaluation is limited to trivial synthetic 2D datasets; there is no evidence the method generalizes to real or higher-dimensional data.', 'Reported improvements are modest (2-13% KL reduction) and may not be practically significant, especially given the doubled computational cost.', 'No comparison to stronger or more sophisticated baselines (e.g., attention, hierarchical, or multi-scale architectures used in diffusion models).', 'No discussion of negative results, failure cases, or broader applicability.', 'The work does not advance the state of the art in generative modeling in any substantive way.']\", \"Originality\": 2, \"Quality\": 2, \"Clarity\": 3, \"Significance\": 1, \"Decision\": \"Reject\"}",
        "novel": true
    },
    {
        "Name": "trajectory_steering_diffusion",
        "Title": "Trajectory-Aware Steering for Diffusion Models",
        "Problem Statement": "Diffusion models lack mechanisms to utilize the directional patterns in denoising trajectories for controlled generation.",
        "Motivation": "The path samples take during denoising contains valuable geometric information about the data structure that could enable more intuitive control.",
        "Proposed Method": "1) Track simple trajectory statistics during sampling (direction changes, velocity) 2) Implement steering by biasing future steps based on trajectory patterns 3) Maintain original sampling as baseline process.",
        "Experiment": "1) Add trajectory tracking to NoiseScheduler 2) Modify step() to implement steering 3) Evaluate on: a) Sample quality (KL) b) Control precision c) Comparison to unsteered sampling 4) Visualize trajectory patterns.",
        "Interestingness": 8,
        "Feasibility": 9,
        "Novelty": 7,
        "lit_review_results": "title: Denoising diffusion probabilistic models are optimally adaptive to   unknown low dimensionality\nabstract: The denoising diffusion probabilistic model (DDPM) has emerged as a mainstream generative model in generative AI. While sharp convergence guarantees have been established for the DDPM, the iteration complexity is, in general, proportional to the ambient data dimension, resulting in overly conservative theory that fails to explain its practical efficiency. This has motivated the recent work Li and Yan (2024a) to investigate how the DDPM can achieve sampling speed-ups through automatic exploitation of intrinsic low dimensionality of data. We strengthen this line of work by demonstrating, in some sense, optimal adaptivity to unknown low dimensionality. For a broad class of data distributions with intrinsic dimension $k$, we prove that the iteration complexity of the DDPM scales nearly linearly with $k$, which is optimal when using KL divergence to measure distributional discrepancy. Notably, our work is closely aligned with the independent concurrent work Potaptchik et al. (2024) -- posted two weeks prior to ours -- in establishing nearly linear-$k$ convergence guarantees for the DDPM.\n\ntitle: Denoising Diffusion Probabilistic Models in Six Simple Steps\nabstract: Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but they have a high barrier-to-entry as they require background knowledge of stochastic differential equations and probability flow. In this note, we distill down the formulation of the DDPM into six simple steps each of which comes with a clear rationale. We assume that the reader is familiar with fundamental topics in machine learning including basic probabilistic modelling, Gaussian distributions, maximum likelihood estimation, and deep learning.\n\ntitle: Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion   Models\nabstract: This paper investigates score-based diffusion models when the underlying target distribution is concentrated on or near low-dimensional manifolds within the higher-dimensional space in which they formally reside, a common characteristic of natural image distributions. Despite previous efforts to understand the data generation process of diffusion models, existing theoretical support remains highly suboptimal in the presence of low-dimensional structure, which we strengthen in this paper. For the popular Denoising Diffusion Probabilistic Model (DDPM), we find that the dependency of the error incurred within each denoising step on the ambient dimension $d$ is in general unavoidable. We further identify a unique design of coefficients that yields a converges rate at the order of $O(k^{2}/\\sqrt{T})$ (up to log factors), where $k$ is the intrinsic dimension of the target distribution and $T$ is the number of steps. This represents the first theoretical demonstration that the DDPM sampler can adapt to unknown low-dimensional structures in the target distribution, highlighting the critical importance of coefficient design. All of this is achieved by a novel set of analysis tools that characterize the algorithmic dynamics in a more deterministic manner.\n\ntitle: Structured Denoising Diffusion Models in Discrete State-Spaces\nabstract: Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.\n\ntitle: Improved Denoising Diffusion Probabilistic Models\nabstract: Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion\n\n",
        "paper": "title: DUALSCALE DIFFUSION: ADAPTIVE FEATURE BAL- ANCING FOR LOW-DIMENSIONAL GENERATIVE MOD- ELS\nabstract: This paper introduces an adaptive dual-scale denoising approach for low- dimensional diffusion models, addressing the challenge of balancing global struc- ture and local detail in generated samples. While diffusion models have shown re- markable success in high-dimensional spaces, their application to low-dimensional data remains crucial for understanding fundamental model behaviors and address- ing real-world applications with inherently low-dimensional data. However, in these spaces, traditional models often struggle to simultaneously capture both macro-level patterns and fine-grained features, leading to suboptimal sample qual- ity. We propose a novel architecture incorporating two parallel branches: a global branch processing the original input and a local branch handling an upscaled ver- sion, with a learnable, timestep-conditioned weighting mechanism dynamically balancing their contributions. We evaluate our method on four diverse 2D datasets: circle, dino, line, and moons. Our results demonstrate significant improvements in sample quality, with KL divergence reductions of up to 12.8% compared to the baseline model. The adaptive weighting successfully adjusts the focus be- tween global and local features across different datasets and denoising stages, as evidenced by our weight evolution analysis. This work not only enhances low-dimensional diffusion models but also provides insights that could inform improvements in higher-dimensional domains, opening new avenues for advancing generative modeling across various applications.",
        "paper_review_results": "{\"paper_id\": \"DualScale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models\", \"Summary\": \"This paper proposes an adaptive dual-scale denoising architecture for low-dimensional (2D) diffusion models. The model consists of two parallel MLP branches\\u2014one processing the original (global) features and the other an upscaled (local) version\\u2014whose outputs are combined via a learnable, timestep-conditioned weighting mechanism. The approach is evaluated on four synthetic 2D datasets, showing modest improvements in KL divergence and visual sample quality compared to a single-branch baseline. Ablation studies and analysis of the learned weights are provided.\", \"Questions\": \"['Can the authors provide theoretical or empirical motivation for why dual-scale adaptive weighting should help in diffusion models, especially in low dimensions?', 'Why is the evaluation limited to synthetic 2D datasets? Have the authors attempted to apply the method to higher-dimensional or real-world data?', 'How does the method compare to a single-branch MLP of equivalent or higher capacity, or to other adaptive feature fusion techniques (e.g., attention, gating, hierarchical models)?', 'Is the upscaling operation critical? What happens if the local branch simply reprocesses the original input, or if the upscaled dimension is varied?', 'Are the reported KL improvements statistically significant and consistent across different random seeds?', 'Can the authors clarify the precise upscaling operation and justify the choice of projection dimension?']\", \"Limitations\": \"['The method is only evaluated on simple 2D synthetic datasets; its applicability to real-world or higher-dimensional data is untested.', 'The computational overhead is significant relative to the modest performance improvement.', 'No theoretical understanding or insight is offered for when dual-scale adaptive weighting is necessary or effective.', 'The adaptive weighting mechanism may overfit to the specific datasets and may not generalize.', 'No discussion of potential negative societal impacts or misuse, though this is less relevant for toy generative modeling.']\", \"Ethical Concerns\": false, \"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Overall\": 2, \"Confidence\": 4, \"Strengths\": \"['The paper is clearly written and well-organized, with explicit architectural descriptions.', 'The proposed adaptive weighting mechanism is simple, interpretable, and easy to implement.', 'Experiments are reproducible, with code availability and detailed experimental settings.', 'Ablation studies and analysis of the weighting mechanism are included.']\", \"Weaknesses\": \"['Technical novelty is minimal: the architecture is a straightforward combination of two MLP branches with learnable weighting, an established idea in deep learning.', 'No theoretical justification or principled motivation is provided for why adaptive dual-scale processing is beneficial in diffusion models.', 'Empirical evaluation is limited to trivial synthetic 2D datasets; there is no evidence the method generalizes to real or higher-dimensional data.', 'Reported improvements are modest (2-13% KL reduction) and may not be practically significant, especially given the doubled computational cost.', 'No comparison to stronger or more sophisticated baselines (e.g., attention, hierarchical, or multi-scale architectures used in diffusion models).', 'No discussion of negative results, failure cases, or broader applicability.', 'The work does not advance the state of the art in generative modeling in any substantive way.']\", \"Originality\": 2, \"Quality\": 2, \"Clarity\": 3, \"Significance\": 1, \"Decision\": \"Reject\"}",
        "novel": true
    }
]