# Title: Manifold-Aware Diffusion via Tangent Space Prediction
# Experiment description: 1) Add tangent space prediction head to MLPDenoiser 2) Modify NoiseScheduler.step() with optional tangent projection 3) Evaluate on all datasets using: a) Sample quality (KL) b) Sample diversity (coverage metrics) c) Sampling efficiency (steps needed).

# Generated Plots Description:

## train_loss.png
This figure shows the training loss curves across all experimental runs (Baseline, Basic Tangent, Enhanced Tangent, Adaptive Mixing) for each of the four datasets (circle, dino, line, moons). The plot consists of a 2x2 grid of subplots, one for each dataset. Key observations:
- The x-axis shows training steps (0 to 10,000)
- The y-axis shows the MSE loss value
- Each method is plotted with a distinct color
- The loss curves show how quickly each method converges
- The Basic Tangent method (run_1) shows slightly faster convergence on complex datasets (dino, moons)
- The Enhanced Tangent method (run_2) shows more unstable training on dino dataset
- All methods eventually reach similar final loss values

## generated_images.png
This figure visualizes the final generated samples from each method. The plot shows:
- Rows correspond to different methods (Baseline to Adaptive Mixing)
- Columns correspond to different datasets (circle, dino, line, moons)
- Each subplot shows 10,000 generated points as a scatter plot
- The ground truth manifold shape is visible in the point distributions
Key observations:
- The Baseline method produces good coverage but slightly blurry edges
- Basic Tangent shows sharper boundaries on complex shapes (dino horns, moon curves)
- Enhanced Tangent produces artifacts on some datasets (e.g., gaps in circle)
- Adaptive Mixing shows intermediate quality between Baseline and Basic Tangent
- All methods struggle most with the dino dataset's complex shape

## Run 0: Baseline
Results: {'circle': {'training_time': 34.17870044708252, 'eval_loss': 0.43525103938853954, 'inference_time': 0.12008213996887207, 'kl_divergence': 0.3389495344946721}, 'dino': {'training_time': 33.63431358337402, 'eval_loss': 0.6616260491673599, 'inference_time': 0.12255191802978516, 'kl_divergence': 1.0424222911130459}, 'line': {'training_time': 33.44674801826477, 'eval_loss': 0.8031019351976302, 'inference_time': 0.11939525604248047, 'kl_divergence': 0.15379776690960525}, 'moons': {'training_time': 33.61119055747986, 'eval_loss': 0.6135321608589738, 'inference_time': 0.12108755111694336, 'kl_divergence': 0.10051708186608005}}
Description: Baseline results with standard diffusion process.

## Run 1: Basic Tangent Projection
Results: {'circle': {'training_time': 34.07218623161316, 'eval_loss': 0.43581021876286363, 'inference_time': 0.2686958312988281, 'kl_divergence': 0.3650265252958061}, 'dino': {'training_time': 33.47750735282898, 'eval_loss': 0.6574622181522877, 'inference_time': 0.1202096939086914, 'kl_divergence': 0.9885227159635471}, 'line': {'training_time': 33.47853374481201, 'eval_loss': 0.8093573130914927, 'inference_time': 0.11849403381347656, 'kl_divergence': 0.14638364153669245}, 'moons': {'training_time': 33.62767314910889, 'eval_loss': 0.6192157949175676, 'inference_time': 0.12060666084289551, 'kl_divergence': 0.10923357486769747}}
Description: Added tangent space prediction head to MLPDenoiser that predicts a single 2D tangent vector. Modified NoiseScheduler.step() to project the noise residual onto the predicted tangent space before applying it. Results show mixed performance - slight improvement on dino dataset (KL from 1.042 to 0.989) but slightly worse on circle (KL from 0.339 to 0.365). The tangent projection appears to help more on complex manifolds (dino) than simple ones (circle, line). Inference time increased slightly due to the extra computation.

## Run 2: Enhanced Tangent Prediction
Results: {'circle': {'training_time': 34.935985803604126, 'eval_loss': 0.43276459634151604, 'inference_time': 0.11905765533447266, 'kl_divergence': 0.3700298277646751}, 'dino': {'training_time': 34.46002721786499, 'eval_loss': 0.6668683214260794, 'inference_time': 0.1228790283203125, 'kl_divergence': 1.06087050311918}, 'line': {'training_time': 33.57499694824219, 'eval_loss': 0.8020179120780867, 'inference_time': 0.12131428718566895, 'kl_divergence': 0.15544966171758057}, 'moons': {'training_time': 34.116273164749146, 'eval_loss': 0.6150744966686229, 'inference_time': 0.12332510948181152, 'kl_divergence': 0.08976062984069062}}
Description: Enhanced tangent space prediction with a 4D tangent basis (2 vectors) and added orthogonality loss during training. Results show this approach performed worse than the simpler tangent projection (Run 1), particularly on the dino dataset where KL divergence increased from 0.989 to 1.061. The moons dataset showed slight improvement (KL from 0.109 to 0.090). The more complex tangent modeling may be over-constraining the diffusion process. Inference time returned to baseline levels as the projection computation was optimized.

## Run 3: Adaptive Tangent Mixing
Results: {'circle': {'training_time': 34.201871395111084, 'eval_loss': 0.44140505714489675, 'inference_time': 0.12017631530761719, 'kl_divergence': 0.36265023802862545}, 'dino': {'training_time': 33.737045764923096, 'eval_loss': 0.6603785343182361, 'inference_time': 0.12021279335021973, 'kl_divergence': 1.0354823660001615}, 'line': {'training_time': 33.673604011535645, 'eval_loss': 0.8069318528370479, 'inference_time': 0.12323164939880371, 'kl_divergence': 0.16904550597626872}, 'moons': {'training_time': 33.98407483100891, 'eval_loss': 0.6120016599250266, 'inference_time': 0.1257617473602295, 'kl_divergence': 0.08432017734869153}}
Description: Implemented adaptive mixing between tangent and standard noise based on timestep (more tangent early, less later). Results show this approach performed similarly to Run 1's basic tangent projection, with slight improvements on moons (KL 0.084 vs 0.109) but worse on line (KL 0.169 vs 0.146). The adaptive mixing didn't provide clear benefits over the simpler approaches, suggesting the tangent projection may work best when applied consistently throughout sampling rather than being mixed. Inference times remained comparable to baseline.
