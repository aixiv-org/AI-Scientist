\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{power2022grokking}
\citation{vaswani2017attention}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{power2022grokking}
\citation{Liu2022TowardsUG}
\citation{Zhang2025IsGA}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}}
\citation{vaswani2017attention,Tay2020EfficientTA}
\citation{ba2016layer}
\citation{Hestness2017DeepLS}
\citation{Kawaguchi2016DeepLW}
\citation{Nakkiran2021TheDB}
\citation{Graves2014NeuralTM}
\citation{goodfellow2016deep}
\citation{loshchilov2017adamw}
\citation{vaswani2017attention}
\citation{Tay2020EfficientTA}
\citation{ba2016layer}
\citation{power2022grokking}
\citation{loshchilov2017adamw}
\citation{kingma2014adam}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related}{{2}{2}{Related Work}{section.2}{}}
\newlabel{sec:related@cref}{{[section][2][]2}{[1][1][]2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Grokking and Generalization}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Transformer Depth}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Algorithmic Learning}{2}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Background}{2}{section.3}\protected@file@percent }
\newlabel{sec:background}{{3}{2}{Background}{section.3}{}}
\newlabel{sec:background@cref}{{[section][3][]3}{[1][2][]2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Transformers and Depth}{2}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Grokking Phenomenon}{2}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Problem Setting}{2}{subsection.3.3}\protected@file@percent }
\citation{vaswani2017attention}
\citation{ba2016layer}
\citation{loshchilov2017adamw}
\@writefile{toc}{\contentsline {section}{\numberline {4}Method}{3}{section.4}\protected@file@percent }
\newlabel{sec:method}{{4}{3}{Method}{section.4}{}}
\newlabel{sec:method@cref}{{[section][4][]4}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model Architecture}{3}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Training Protocol}{3}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Evaluation}{3}{subsection.4.3}\protected@file@percent }
\citation{vaswani2017attention}
\citation{ba2016layer}
\citation{loshchilov2017adamw}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Setup}{4}{section.5}\protected@file@percent }
\newlabel{sec:experimental}{{5}{4}{Experimental Setup}{section.5}{}}
\newlabel{sec:experimental@cref}{{[section][5][]5}{[1][3][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Tasks and Data}{4}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Model Implementation}{4}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Training Protocol}{4}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Evaluation}{4}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{4}{section.6}\protected@file@percent }
\newlabel{sec:results}{{6}{4}{Results}{section.6}{}}
\newlabel{sec:results@cref}{{[section][6][]6}{[1][4][]4}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance by depth (mean ± std error)\relax }}{5}{table.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:results}{{1}{5}{Performance by depth (mean ± std error)\relax }{table.caption.1}{}}
\newlabel{tab:results@cref}{{[table][1][]1}{[1][4][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Arithmetic Performance}{5}{subsection.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Validation accuracy across arithmetic tasks. 1-layer models (blue) show faster convergence to 99\% threshold (dashed line).\relax }}{5}{figure.caption.2}\protected@file@percent }
\newlabel{fig:arithmetic}{{1}{5}{Validation accuracy across arithmetic tasks. 1-layer models (blue) show faster convergence to 99\% threshold (dashed line).\relax }{figure.caption.2}{}}
\newlabel{fig:arithmetic@cref}{{[figure][1][]1}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Permutation Learning}{5}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Limitations}{5}{subsection.6.3}\protected@file@percent }
\citation{goodfellow2016deep}
\citation{power2022grokking}
\citation{vaswani2017attention}
\citation{lu2024aiscientist}
\bibstyle{iclr2024_conference}
\bibdata{references}
\bibcite{ba2016layer}{{1}{2016}{{Ba et~al.}}{{Ba, Kiros, and Hinton}}}
\bibcite{goodfellow2016deep}{{2}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, Courville, and Bengio}}}
\bibcite{Graves2014NeuralTM}{{3}{2014}{{Graves et~al.}}{{Graves, Wayne, and Danihelka}}}
\bibcite{Hestness2017DeepLS}{{4}{2017}{{Hestness et~al.}}{{Hestness, Narang, Ardalani, Diamos, Jun, Kianinejad, Patwary, Yang, and Zhou}}}
\bibcite{Kawaguchi2016DeepLW}{{5}{2016}{{Kawaguchi}}{{}}}
\bibcite{kingma2014adam}{{6}{2014}{{Kingma \& Ba}}{{Kingma and Ba}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Depth's detrimental effect on permutation learning. Despite similar training accuracy, deeper models generalize poorly.\relax }}{6}{figure.caption.3}\protected@file@percent }
\newlabel{fig:permutation}{{2}{6}{Depth's detrimental effect on permutation learning. Despite similar training accuracy, deeper models generalize poorly.\relax }{figure.caption.3}{}}
\newlabel{fig:permutation@cref}{{[figure][2][]2}{[1][5][]6}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions and Future Work}{6}{section.7}\protected@file@percent }
\newlabel{sec:conclusion}{{7}{6}{Conclusions and Future Work}{section.7}{}}
\newlabel{sec:conclusion@cref}{{[section][7][]7}{[1][5][]6}}
\bibcite{Liu2022TowardsUG}{{7}{2022}{{Liu et~al.}}{{Liu, Kitouni, Nolte, Michaud, Tegmark, and Williams}}}
\bibcite{loshchilov2017adamw}{{8}{2017}{{Loshchilov \& Hutter}}{{Loshchilov and Hutter}}}
\bibcite{lu2024aiscientist}{{9}{2024}{{Lu et~al.}}{{Lu, Lu, Lange, Foerster, Clune, and Ha}}}
\bibcite{Nakkiran2021TheDB}{{10}{2021}{{Nakkiran et~al.}}{{Nakkiran, Neyshabur, and Sedghi}}}
\bibcite{power2022grokking}{{11}{2022}{{Power et~al.}}{{Power, Burda, Edwards, Babuschkin, and Misra}}}
\bibcite{Tay2020EfficientTA}{{12}{2020}{{Tay et~al.}}{{Tay, Dehghani, Bahri, and Metzler}}}
\bibcite{vaswani2017attention}{{13}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{Zhang2025IsGA}{{14}{2025}{{Zhang et~al.}}{{Zhang, Shang, Yang, and Zhang}}}
\ttl@finishall
\gdef \@abspage@last{7}
