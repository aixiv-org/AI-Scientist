\begin{thebibliography}{14}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
\newblock \emph{Deep learning}, volume~1.
\newblock MIT Press, 2016.

\bibitem[Graves et~al.(2014)Graves, Wayne, and Danihelka]{Graves2014NeuralTM}
Alex Graves, Greg Wayne, and Ivo Danihelka.
\newblock Neural turing machines.
\newblock \emph{ArXiv}, abs/1410.5401, 2014.

\bibitem[Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun,
  Kianinejad, Patwary, Yang, and Zhou]{Hestness2017DeepLS}
Joel Hestness, Sharan Narang, Newsha Ardalani, G.~Diamos, Heewoo Jun, Hassan
  Kianinejad, Md. Mostofa~Ali Patwary, Yang Yang, and Yanqi Zhou.
\newblock Deep learning scaling is predictable, empirically.
\newblock \emph{ArXiv}, abs/1712.00409, 2017.

\bibitem[Kawaguchi(2016)]{Kawaguchi2016DeepLW}
Kenji Kawaguchi.
\newblock Deep learning without poor local minima.
\newblock \emph{ArXiv}, abs/1605.07110, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Liu et~al.(2022)Liu, Kitouni, Nolte, Michaud, Tegmark, and
  Williams]{Liu2022TowardsUG}
Ziming Liu, Ouail Kitouni, Niklas Nolte, Eric~J. Michaud, Max Tegmark, and Mike
  Williams.
\newblock Towards understanding grokking: An effective theory of representation
  learning.
\newblock \emph{ArXiv}, abs/2205.10343, 2022.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017adamw}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Lu et~al.(2024)Lu, Lu, Lange, Foerster, Clune, and
  Ha]{lu2024aiscientist}
Chris Lu, Cong Lu, Robert~Tjarko Lange, Jakob Foerster, Jeff Clune, and David
  Ha.
\newblock The {AI} {S}cientist: Towards fully automated open-ended scientific
  discovery.
\newblock \emph{arXiv preprint arXiv:2408.06292}, 2024.

\bibitem[Nakkiran et~al.(2021)Nakkiran, Neyshabur, and
  Sedghi]{Nakkiran2021TheDB}
Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi.
\newblock The deep bootstrap framework: Good online learners are good offline
  generalizers.
\newblock 2021.

\bibitem[Power et~al.(2022)Power, Burda, Edwards, Babuschkin, and
  Misra]{power2022grokking}
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.
\newblock Grokking: Generalization beyond overfitting on small algorithmic
  datasets.
\newblock \emph{arXiv preprint arXiv:2201.02177}, 2022.

\bibitem[Tay et~al.(2020)Tay, Dehghani, Bahri, and Metzler]{Tay2020EfficientTA}
Yi~Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
\newblock Efficient transformers: A survey.
\newblock \emph{ACM Computing Surveys}, 55:\penalty0 1 -- 28, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Zhang et~al.(2025)Zhang, Shang, Yang, and Zhang]{Zhang2025IsGA}
Xiaotian Zhang, Yue Shang, Entao Yang, and Ge~Zhang.
\newblock Is grokking a computational glass relaxation?
\newblock \emph{ArXiv}, abs/2505.11411, 2025.

\end{thebibliography}
