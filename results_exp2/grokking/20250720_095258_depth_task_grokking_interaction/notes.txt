# Title: Depth-Task Interactions in Grokking: How Transformer Depth Affects Generalization Across Algorithmic Tasks
# Experiment description: Run experiments with layer counts (1, 2, 4) across all available tasks (x_plus_y, x_minus_y, x_div_y). Track and compare: (1) steps to grokking, (2) final generalization accuracy, and (3) training stability metrics for each depth-task combination.

## Run 0: Baseline (2-layer)
Results: {'x_div_y': {'final_train_loss_mean': 0.0051286482873062296, 'final_val_loss_mean': 0.0058011244982481, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 4273.333333333333}, 'x_minus_y': {'final_train_loss_mean': 0.012638177489861846, 'final_val_loss_mean': 0.012431656320889791, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 4276.666666666667}, 'x_plus_y': {'final_train_loss_mean': 0.003282314476867517, 'final_val_loss_mean': 1.4834302957945813, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 0.735595703125, 'step_val_acc_99_mean': 2410.0}, 'permutation': {'final_train_loss_mean': 0.06612378917634487, 'final_val_loss_mean': 5.423408031463623, 'final_train_acc_mean': 0.9911458293596903, 'final_val_acc_mean': 0.273193359375, 'step_val_acc_99_mean': 7500.0}}
Description: Baseline results with 2-layer transformer.

## Run 1: 1-layer transformer
Results: {'x_div_y': {'final_train_loss_mean': 0.005025311140343547, 'final_val_loss_mean': 0.005915137783934672, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 3793.3333333333335}, 'x_minus_y': {'final_train_loss_mean': 0.004242694781472285, 'final_val_loss_mean': 0.00534682363892595, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 3483.3333333333335}, 'x_plus_y': {'final_train_loss_mean': 0.0028838004606465497, 'final_val_loss_mean': 0.0030028860395153365, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 2260.0}, 'permutation': {'final_train_loss_mean': 0.016640032272941124, 'final_val_loss_mean': 2.4439183423916497, 'final_train_acc_mean': 0.998632808526357, 'final_val_acc_mean': 0.6647135416666666, 'step_val_acc_99_mean': 7500.0}}
Description: 1-layer transformer showed surprising improvements over baseline - faster grokking (2260-3793 steps vs 2410-4276) and better generalization (100% val acc on arithmetic tasks vs 73.6% for x_plus_y in baseline). The permutation task also improved from 27.3% to 66.5% val accuracy, though still didn't fully grok by step 7500. This suggests shallower networks may be more effective for these algorithmic tasks.

## Run 2: 4-layer transformer
Results: {'x_div_y': {'final_train_loss_mean': 0.03861029647911588, 'final_val_loss_mean': 0.03780503757297993, 'final_train_acc_mean': 0.998632808526357, 'final_val_acc_mean': 0.9988606770833334, 'step_val_acc_99_mean': 4236.666666666667}, 'x_minus_y': {'final_train_loss_mean': 0.011308611991504828, 'final_val_loss_mean': 0.01463261153548956, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 4633.333333333333}, 'x_plus_y': {'final_train_loss_mean': 0.0039054184841612973, 'final_val_loss_mean': 0.00418046109067897, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 2516.6666666666665}, 'permutation': {'final_train_loss_mean': 0.017238991955916088, 'final_val_loss_mean': 5.594533522923787, 'final_train_acc_mean': 0.9998697837193807, 'final_val_acc_mean': 0.14347330729166666, 'step_val_acc_99_mean': 7500.0}}
Description: 4-layer transformer showed mixed results - maintaining strong performance on arithmetic tasks (100% val acc for x_plus_y/x_minus_y, 99.9% for x_div_y) but with slightly slower grokking (2516-4633 steps) compared to 1-layer. Most notably, permutation task performance dropped dramatically to 14.3% val accuracy, suggesting increased depth may hinder learning of more complex operations while providing minimal benefits for simpler arithmetic tasks.

## Plot Descriptions

### Training Loss Plots (train_loss_*.png)
- Shows model convergence during training across different transformer depths
- Key observations: 
  - 1-layer models converge fastest across all tasks
  - 4-layer shows more volatile training for x_div_y and permutation tasks
- Files:
  - train_loss_x_div_y.png
  - train_loss_x_minus_y.png
  - train_loss_x_plus_y.png
  - train_loss_permutation.png

### Validation Loss Plots (val_loss_*.png)
- Measures generalization performance during training
- Key observations:
  - 1-layer maintains lowest validation loss throughout training
  - 4-layer shows instability in validation loss for permutation task
- Files:
  - val_loss_x_div_y.png
  - val_loss_x_minus_y.png
  - val_loss_x_plus_y.png
  - val_loss_permutation.png

### Training Accuracy Plots (train_acc_*.png)
- Shows training set mastery over time
- Key observations:
  - All depths achieve near-perfect training accuracy
  - 1-layer reaches 100% accuracy fastest
- Files:
  - train_acc_x_div_y.png
  - train_acc_x_minus_y.png
  - train_acc_x_plus_y.png
  - train_acc_permutation.png

### Validation Accuracy Plots (val_acc_*.png)
- Most important plots showing grokking behavior
- Includes 99% accuracy threshold line (dashed gray)
- Key observations:
  - 1-layer shows fastest grokking (earliest crossing of 99% line)
  - 4-layer fails to grok permutation task
  - x_plus_y shows dramatic difference between depths
- Files:
  - val_acc_x_div_y.png
  - val_acc_x_minus_y.png
  - val_acc_x_plus_y.png
  - val_acc_permutation.png

### General Findings:
1. Shallower networks (1-layer) consistently outperform deeper ones
2. Depth benefits diminish for simpler arithmetic tasks
3. Increased depth actively harms performance on complex tasks (permutation)
4. Training curves show depth affects learning dynamics beyond just final performance
