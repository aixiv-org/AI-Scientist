%% LaTeX2e file `references.bib'
%% generated by the `filecontents' environment
%% from source `template' on 2025/07/20.
%%
@article{lu2024aiscientist,
  title={The {AI} {S}cientist: Towards Fully Automated Open-Ended Scientific Discovery},
  author={Lu, Chris and Lu, Cong and Lange, Robert Tjarko and Foerster, Jakob and Clune, Jeff and Ha, David},
  journal={arXiv preprint arXiv:2408.06292},
  year={2024}
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  year={2016},
  publisher={MIT Press}
}

@article{power2022grokking,
  title={Grokking: Generalization beyond overfitting on small algorithmic datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{loshchilov2017adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@Article{Jastrzebski2020TheBP,
 author = {Stanislaw Jastrzebski and Maciej Szymczak and Stanislav Fort and Devansh Arpit and J. Tabor and Kyunghyun Cho and Krzysztof J. Geras},
 booktitle = {International Conference on Learning Representations},
 journal = {ArXiv},
 title = {The Break-Even Point on Optimization Trajectories of Deep Neural Networks},
 volume = {abs/2002.09572},
 year = {2020}
}


@Article{Polyak1964SomeMO,
 author = {Boris Polyak},
 journal = {Ussr Computational Mathematics and Mathematical Physics},
 pages = {1-17},
 title = {Some methods of speeding up the convergence of iteration methods},
 volume = {4},
 year = {1964}
}


@Article{Zhang2016UnderstandingDL,
 author = {Chiyuan Zhang and Samy Bengio and Moritz Hardt and B. Recht and O. Vinyals},
 booktitle = {International Conference on Learning Representations},
 journal = {ArXiv},
 title = {Understanding deep learning requires rethinking generalization},
 volume = {abs/1611.03530},
 year = {2016}
}


@Article{Thomas2019OnTI,
 author = {Valentin Thomas and Fabian Pedregosa and B. V. Merrienboer and Pierre-Antoine Mangazol and Yoshua Bengio and Nicolas Le Roux},
 booktitle = {International Conference on Artificial Intelligence and Statistics},
 pages = {3503-3513},
 title = {On the interplay between noise and curvature and its effect on optimization and generalization},
 year = {2019}
}


@Article{Smith2017ABP,
 author = {Samuel L. Smith and Quoc V. Le},
 booktitle = {International Conference on Learning Representations},
 journal = {ArXiv},
 title = {A Bayesian Perspective on Generalization and Stochastic Gradient Descent},
 volume = {abs/1710.06451},
 year = {2017}
}


@Article{Xie2020AdaptiveID,
 author = {Zeke Xie and Xinrui Wang and Huishuai Zhang and Issei Sato and Masashi Sugiyama},
 booktitle = {International Conference on Machine Learning},
 pages = {24430-24459},
 title = {Adaptive Inertia: Disentangling the Effects of Adaptive Learning Rate and Momentum},
 year = {2020}
}


@Article{Xie2022AdanAN,
 author = {Xingyu Xie and Pan Zhou and Huan Li and Zhouchen Lin and Shuicheng Yan},
 booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 pages = {9508-9520},
 title = {Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models},
 volume = {46},
 year = {2022}
}


@Article{Yan2018AUA,
 author = {Yan Yan and Tianbao Yang and Zhe Li and Qihang Lin and Yi Yang},
 booktitle = {International Joint Conference on Artificial Intelligence},
 journal = {ArXiv},
 title = {A Unified Analysis of Stochastic Momentum Methods for Deep Learning},
 volume = {abs/1808.10396},
 year = {2018}
}


@Article{Ge2015EscapingFS,
 author = {Rong Ge and Furong Huang and Chi Jin and Yang Yuan},
 booktitle = {Annual Conference Computational Learning Theory},
 journal = {ArXiv},
 title = {Escaping From Saddle Points - Online Stochastic Gradient for Tensor Decomposition},
 volume = {abs/1503.02101},
 year = {2015}
}


@Article{Nesterov1983AMF,
 author = {Y. Nesterov},
 journal = {Proceedings of the USSR Academy of Sciences},
 pages = {543-547},
 title = {A method for solving the convex programming problem with convergence rate O(1/k^2)},
 volume = {269},
 year = {1983}
}

