{
    "Name": "attention_phase_transition_grokking",
    "Title": "Attention Phase Transitions in Grokking: Comparing Memorization vs Generalization Patterns",
    "Problem Statement": "How do Transformer attention patterns fundamentally differ between memorization and generalization phases during grokking?",
    "Motivation": "Identifying distinct attention mechanisms underlying memorization versus genuine algorithmic understanding could reveal how neural networks transition between these learning modes and provide biomarkers for impending grokking.",
    "Proposed Method": "Quantitatively compare attention patterns before and after grokking occurs, identifying phase-specific attention head specializations and their relationship to generalization performance.",
    "Experiment": "1) Modify Transformer to log attention weights. 2) Train models while tracking validation accuracy to identify grokking point. 3) Extract and cluster attention patterns from: a) pure memorization phase, b) transition period, c) post-grokking phase. 4) Statistically compare attention distributions across phases. 5) Correlate specific attention patterns with generalization onset.",
    "Interestingness": 9,
    "Feasibility": 9,
    "Novelty": 8,
    "lit_review_results": "title: Deep Grokking: Would Deep Neural Networks Generalize Better?\nabstract: Recent research on the grokking phenomenon has illuminated the intricacies of neural networks' training dynamics and their generalization behaviors. Grokking refers to a sharp rise of the network's generalization accuracy on the test set, which occurs long after an extended overfitting phase, during which the network perfectly fits the training set. While the existing research primarily focus on shallow networks such as 2-layer MLP and 1-layer Transformer, we explore grokking on deep networks (e.g. 12-layer MLP). We empirically replicate the phenomenon and find that deep neural networks can be more susceptible to grokking than its shallower counterparts. Meanwhile, we observe an intriguing multi-stage generalization phenomenon when increase the depth of the MLP model where the test accuracy exhibits a secondary surge, which is scarcely seen on shallow models. We further uncover compelling correspondences between the decreasing of feature ranks and the phase transition from overfitting to the generalization stage during grokking. Additionally, we find that the multi-stage generalization phenomenon often aligns with a double-descent pattern in feature ranks. These observations suggest that internal feature rank could serve as a more promising indicator of the model's generalization behavior compared to the weight-norm. We believe our work is the first one to dive into grokking in deep neural networks, and investigate the relationship of feature rank and generalization performance.\n\ntitle: Exploring Grokking: Experimental and Mechanistic Investigations\nabstract: The phenomenon of grokking in over-parameterized neural networks has garnered significant interest. It involves the neural network initially memorizing the training set with zero training error and near-random test error. Subsequent prolonged training leads to a sharp transition from no generalization to perfect generalization. Our study comprises extensive experiments and an exploration of the research behind the mechanism of grokking. Through experiments, we gained insights into its behavior concerning the training data fraction, the model, and the optimization. The mechanism of grokking has been a subject of various viewpoints proposed by researchers, and we introduce some of these perspectives.\n\ntitle: To grok or not to grok: Disentangling generalization and memorization on   corrupted algorithmic datasets\nabstract: Robust generalization is a major challenge in deep learning, particularly when the number of trainable parameters is very large. In general, it is very difficult to know if the network has memorized a particular set of examples or understood the underlying rule (or both). Motivated by this challenge, we study an interpretable model where generalizing representations are understood analytically, and are easily distinguishable from the memorizing ones. Namely, we consider multi-layer perceptron (MLP) and Transformer architectures trained on modular arithmetic tasks, where ($\\xi \\cdot 100\\%$) of labels are corrupted (\\emph{i.e.} some results of the modular operations in the training set are incorrect). We show that (i) it is possible for the network to memorize the corrupted labels \\emph{and} achieve $100\\%$ generalization at the same time; (ii) the memorizing neurons can be identified and pruned, lowering the accuracy on corrupted data and improving the accuracy on uncorrupted data; (iii) regularization methods such as weight decay, dropout and BatchNorm force the network to ignore the corrupted data during optimization, and achieve $100\\%$ accuracy on the uncorrupted dataset; and (iv) the effect of these regularization methods is (``mechanistically'') interpretable: weight decay and dropout force all the neurons to learn generalizing representations, while BatchNorm de-amplifies the output of memorizing neurons and amplifies the output of the generalizing ones. Finally, we show that in the presence of regularization, the training dynamics involves two consecutive stages: first, the network undergoes \\emph{grokking} dynamics reaching high train \\emph{and} test accuracy; second, it unlearns the memorizing representations, where the train accuracy suddenly jumps from $100\\%$ to $100 (1-\\xi)\\%$.\n\ntitle: Grokking in Linear Estimators -- A Solvable Model that Groks without   Understanding\nabstract: Grokking is the intriguing phenomenon where a model learns to generalize long after it has fit the training data. We show both analytically and numerically that grokking can surprisingly occur in linear networks performing linear tasks in a simple teacher-student setup with Gaussian inputs. In this setting, the full training dynamics is derived in terms of the training and generalization data covariance matrix. We present exact predictions on how the grokking time depends on input and output dimensionality, train sample size, regularization, and network initialization. We demonstrate that the sharp increase in generalization accuracy may not imply a transition from \"memorization\" to \"understanding\", but can simply be an artifact of the accuracy measure. We provide empirical verification for our calculations, along with preliminary results indicating that some predictions also hold for deeper networks, with non-linear activations.\n\ntitle: NeuralGrok: Accelerate Grokking by Neural Gradient Transformation\nabstract: Grokking is proposed and widely studied as an intricate phenomenon in which generalization is achieved after a long-lasting period of overfitting. In this work, we propose NeuralGrok, a novel gradient-based approach that learns an optimal gradient transformation to accelerate the generalization of transformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary module (e.g., an MLP block) in conjunction with the base model. This module dynamically modulates the influence of individual gradient components based on their contribution to generalization, guided by a bilevel optimization algorithm. Our extensive experiments demonstrate that NeuralGrok significantly accelerates generalization, particularly in challenging arithmetic tasks. We also show that NeuralGrok promotes a more stable training paradigm, constantly reducing the model's complexity, while traditional regularization methods, such as weight decay, can introduce substantial instability and impede generalization. We further investigate the intrinsic model complexity leveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that NeuralGrok effectively facilitates generalization by reducing the model complexity. We offer valuable insights on the grokking phenomenon of Transformer models, which encourages a deeper understanding of the fundamental principles governing generalization ability.\n\n",
    "paper": "title: UNLOCKING GROKKING: A COMPARATIVE STUDY OF WEIGHT INITIALIZATION STRATEGIES IN TRANS- FORMER MODELS\nabstract: This paper investigates the impact of weight initialization strategies on the grokking phenomenon in Transformer models, addressing the challenge of understanding and optimizing neural network learning dynamics. Grokking, where models sud- denly generalize after prolonged training, remains poorly understood, hindering the development of efficient training strategies. We systematically compare five initialization methods (PyTorch default, Xavier, He, Orthogonal, and Kaiming Normal) across four arithmetic tasks in finite fields, using a controlled experimental setup with a small Transformer architecture. Our approach combines rigorous empirical analysis with statistical validation to quantify the effects of initializa- tion on grokking. Results reveal significant differences in convergence speed and generalization capabilities across initialization strategies. Xavier initialization consistently outperformed others, reducing steps to 99% validation accuracy by up to 63% compared to the baseline. Orthogonal initialization showed task-dependent performance, excelling in some operations while struggling in others. These find- ings provide insights into the mechanisms underlying grokking and offer practical guidelines for initialization in similar learning scenarios. Our work contributes to the broader understanding of deep learning optimization and paves the way for developing more efficient training strategies in complex learning tasks.",
    "paper_review_results": "{\"paper_id\": \"Unlocking Grokking: A Comparative Study of Weight Initialization Strategies in Transformer Models\", \"Summary\": \"This paper presents a systematic empirical study of the impact of five standard weight initialization strategies (PyTorch default, Xavier, He, Orthogonal, Kaiming Normal) on the grokking phenomenon in small Transformer models trained on simple arithmetic tasks in finite fields. The main finding is that Xavier and Orthogonal initializations lead to faster and more reliable grokking (measured as steps to 99% validation accuracy) compared to other initializations. The study is limited to small models and toy tasks, and is entirely empirical.\", \"Questions\": \"['Can the authors provide a mechanistic or theoretical explanation for why certain initialization strategies facilitate grokking, beyond empirical observations?', 'Do the observed effects generalize to larger Transformer models or to real-world tasks?', 'Why were only three random seeds used? Can results be replicated with more seeds to ensure robustness?', 'Can the authors clarify the missing figures and provide code or detailed pseudocode for full reproducibility?', 'Are there broader implications or actionable insights for the deep learning community beyond the specific toy setting studied?']\", \"Limitations\": \"['Results are limited to small models and simple tasks; findings may not generalize to larger models or practical domains.', 'No theoretical or mechanistic explanation for the observed differences between initialization strategies.', 'Statistical robustness is limited by the use of only three seeds.', 'No code or resource release, limiting reproducibility.', 'No discussion of societal impact, ethical considerations, or broader limitations.']\", \"Ethical Concerns\": false, \"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Overall\": 2, \"Confidence\": 4, \"Strengths\": \"['Addresses the timely and interesting phenomenon of grokking in deep learning.', 'Systematic and controlled empirical comparison of multiple initialization strategies.', 'Clear experimental setup, with reporting of statistical measures (mean, confidence intervals).', 'Experimental protocols are described in detail, aiding reproducibility in principle.']\", \"Weaknesses\": \"['The contribution is narrow and highly incremental, amounting to an empirical sweep of standard initialization methods with no new theory, methodology, or deep analysis.', 'Findings are unsurprising and align with established knowledge that initialization affects optimization and convergence.', 'Experiments are limited to very small models and synthetic arithmetic tasks; there is no evidence for generality to larger models or real-world tasks.', 'Only three random seeds are used per configuration, limiting statistical robustness.', 'No mechanistic or theoretical explanation is provided for why certain initializations facilitate grokking.', 'Figures are missing from the submission, and the presentation is incomplete.', 'No code or resources are provided for reproducibility.', 'Related work is only superficially covered, and connections to recent advances in grokking theory or practice are lacking.', 'No discussion of broader implications, limitations, or societal impact.']\", \"Originality\": 1, \"Quality\": 2, \"Clarity\": 3, \"Significance\": 1, \"Decision\": \"Reject\"}",
    "novel": true
}