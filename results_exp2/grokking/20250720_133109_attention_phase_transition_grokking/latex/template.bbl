\begin{thebibliography}{11}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Advani \& Saxe(2017)Advani and Saxe]{Advani2017HighdimensionalDO}
Madhu~S. Advani and Andrew~M. Saxe.
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock \emph{Neural Networks}, 132:\penalty0 428 -- 446, 2017.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Clark et~al.(2019)Clark, Khandelwal, Levy, and
  Manning]{Clark2019WhatDB}
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher~D. Manning.
\newblock What does bert look at? an analysis of bertâ€™s attention.
\newblock pp.\  276--286, 2019.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017adamw}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Lu et~al.(2024)Lu, Lu, Lange, Foerster, Clune, and
  Ha]{lu2024aiscientist}
Chris Lu, Cong Lu, Robert~Tjarko Lange, Jakob Foerster, Jeff Clune, and David
  Ha.
\newblock The {AI} {S}cientist: Towards fully automated open-ended scientific
  discovery.
\newblock \emph{arXiv preprint arXiv:2408.06292}, 2024.

\bibitem[Power et~al.(2022)Power, Burda, Edwards, Babuschkin, and
  Misra]{power2022grokking}
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.
\newblock Grokking: Generalization beyond overfitting on small algorithmic
  datasets.
\newblock \emph{arXiv preprint arXiv:2201.02177}, 2022.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and Ganguli]{Saxe2013ExactST}
Andrew~M. Saxe, James~L. McClelland, and S.~Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{CoRR}, abs/1312.6120, 2013.

\bibitem[Tamai et~al.(2023)Tamai, Okubo, Duy, Natori, and
  Todo]{Tamai2023UniversalSL}
Keiichi Tamai, T.~Okubo, T.~V. Duy, N.~Natori, and S.~Todo.
\newblock Universal scaling laws of absorbing phase transitions in artificial
  deep neural networks.
\newblock 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Voita et~al.(2019)Voita, Talbot, Moiseev, Sennrich, and
  Titov]{Voita2019AnalyzingMS}
Elena Voita, David Talbot, F.~Moiseev, Rico Sennrich, and Ivan Titov.
\newblock Analyzing multi-head self-attention: Specialized heads do the heavy
  lifting, the rest can be pruned.
\newblock \emph{ArXiv}, abs/1905.09418, 2019.

\bibitem[Zhai et~al.(2023)Zhai, Likhomanenko, Littwin, Busbridge, Ramapuram,
  Zhang, Gu, and Susskind]{Zhai2023StabilizingTT}
Shuangfei Zhai, T.~Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram,
  Yizhe Zhang, Jiatao Gu, and J.~Susskind.
\newblock Stabilizing transformer training by preventing attention entropy
  collapse.
\newblock \emph{ArXiv}, abs/2303.06296, 2023.

\end{thebibliography}
