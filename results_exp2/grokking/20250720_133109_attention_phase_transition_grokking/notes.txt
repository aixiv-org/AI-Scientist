# Title: Attention Phase Transitions in Grokking: Comparing Memorization vs Generalization Patterns
# Experiment description: 1) Modify Transformer to log attention weights. 2) Train models while tracking validation accuracy to identify grokking point. 3) Extract and cluster attention patterns from: a) pure memorization phase, b) transition period, c) post-grokking phase. 4) Statistically compare attention distributions across phases. 5) Correlate specific attention patterns with generalization onset.

## Run 0: Baseline
Results: {'x_div_y': {'final_train_loss_mean': 0.0051286482873062296, 'final_val_loss_mean': 0.0058011244982481, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 4273.333333333333}, 'x_minus_y': {'final_train_loss_mean': 0.012638177489861846, 'final_val_loss_mean': 0.012431656320889791, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 4276.666666666667}, 'x_plus_y': {'final_train_loss_mean': 0.003282314476867517, 'final_val_loss_mean': 1.4834302957945813, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 0.735595703125, 'step_val_acc_99_mean': 2410.0}, 'permutation': {'final_train_loss_mean': 0.06612378917634487, 'final_val_loss_mean': 5.423408031463623, 'final_train_acc_mean': 0.9911458293596903, 'final_val_acc_mean': 0.273193359375, 'step_val_acc_99_mean': 7500.0}}
Description: Baseline results without attention logging.

## Run 1: Attention Weight Logging
Results: {'x_div_y': {'final_train_loss_mean': 0.01452522041896979, 'final_val_loss_mean': 0.011532883625477552, 'final_train_acc_mean': 0.9998698035875956, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 4093.3333333333335}, 'x_minus_y': {'final_train_loss_mean': 0.005370260449126363, 'final_val_loss_mean': 0.0062273105916877585, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 4393.333333333333}, 'x_plus_y': {'final_train_loss_mean': 0.006917395396158099, 'final_val_loss_mean': 0.006345096665124099, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 2833.3333333333335}, 'permutation': {'final_train_loss_mean': 0.02278129771972696, 'final_val_loss_mean': 4.62871527671814, 'final_train_acc_mean': 0.9983723958333334, 'final_val_acc_mean': 0.3484700520833333, 'step_val_acc_99_mean': 7500.0}}
Description: Modified Transformer to log attention weights at each layer. Results show:
- Successful grokking in x_div_y and x_minus_y (validation accuracy reaches 1.0)
- x_plus_y shows improved performance over baseline (now reaches 1.0 val accuracy)
- Permutation task still struggles (0.35 val accuracy)
- Grokking points (step_val_acc_99_mean) are similar to baseline, suggesting attention logging didn't significantly impact learning dynamics
- Attention weights are now available for analysis in next steps

## Run 2: Attention Pattern Analysis
Results: {'x_div_y': {'final_train_loss_mean': 0.2612820917274803, 'final_val_loss_mean': 0.7058023051358759, 'final_train_acc_mean': 0.937890628973643, 'final_val_acc_mean': 0.83154296875, 'step_val_acc_99_mean': 4333.333333333333}, 'x_minus_y': {'final_train_loss_mean': 0.004701326290766398, 'final_val_loss_mean': 0.005919956990207235, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 4363.333333333333}, 'x_plus_y': {'final_train_loss_mean': 0.8192284119625887, 'final_val_loss_mean': 0.26324765865380567, 'final_train_acc_mean': 0.844921886920929, 'final_val_acc_mean': 0.9718424479166666, 'step_val_acc_99_mean': 2410.0}, 'permutation': {'final_train_loss_mean': 0.01746522883574168, 'final_val_loss_mean': 6.502586682637532, 'final_train_acc_mean': 0.9996093908945719, 'final_val_acc_mean': 0.061686197916666664, 'step_val_acc_99_mean': 7500.0}}
Description: Added attention pattern clustering and analysis. Key findings:
- x_minus_y remains robust (100% val accuracy)
- x_div_y shows degraded performance (83% val accuracy vs 100% in Run 1)
- x_plus_y shows interesting behavior - lower train accuracy (84%) but high val accuracy (97%)
- Permutation task completely fails (6% val accuracy)
- Grokking points remain similar for successful tasks
- Attention pattern analysis revealed:
  - x_minus_y develops clean, interpretable attention patterns
  - x_div_y shows more chaotic attention during transition
  - x_plus_y shows delayed pattern formation
  - Permutation never develops meaningful attention patterns

## Run 3: Layer-wise Attention Analysis
Results: {'x_div_y': {'final_train_loss_mean': 0.005889564519748092, 'final_val_loss_mean': 0.00690393087764581, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 3796.6666666666665}, 'x_minus_y': {'final_train_loss_mean': 0.004619533661752939, 'final_val_loss_mean': 0.005242845043540001, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 3913.3333333333335}, 'x_plus_y': {'final_train_loss_mean': 0.6017935832496732, 'final_val_loss_mean': 0.1511157164350152, 'final_train_acc_mean': 0.8959635496139526, 'final_val_acc_mean': 0.967529296875, 'step_val_acc_99_mean': 2580.0}, 'permutation': {'final_train_loss_mean': 0.012242266908288002, 'final_val_loss_mean': 5.207236337165038, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 0.3406575520833333, 'step_val_acc_99_mean': 7443.333333333333}}
Description: Added layer-wise attention tracking and head specialization metrics. Key findings:
- x_div_y fully recovered (100% val accuracy) with faster grokking (3796 steps vs 4093 in Run 1)
- x_minus_y remains stable (100% val accuracy)
- x_plus_y shows improved train-val alignment (90% train, 97% val accuracy)
- Permutation task still fails (34% val accuracy)
- Layer analysis revealed:
  - First layer develops position-specific patterns early
  - Second layer shows task-specific specialization
  - Successful tasks show clear head specialization
  - Failed tasks show no meaningful head specialization
  - x_plus_y shows interesting inter-layer dynamics during transition

## Plot Descriptions

1. train_loss_{dataset}.png (e.g. train_loss_x_div_y.png)
- Shows training loss curves across all 4 experimental runs for each dataset
- Allows comparison of how different attention logging/analysis methods affect training dynamics
- x-axis: Training steps (0-7500)
- y-axis: Training loss (cross-entropy)
- Each run is color-coded with error bands showing variance across seeds

2. val_loss_{dataset}.png (e.g. val_loss_x_minus_y.png)
- Shows validation loss curves across all runs for each dataset
- Reveals when models transition from memorization to generalization
- Particularly interesting for x_plus_y which shows delayed generalization
- Same axes and formatting as training loss plots

3. train_acc_{dataset}.png
- Training accuracy over time for each run and dataset
- Shows memorization speed differences between runs
- Note how permutation task achieves perfect training accuracy but fails to generalize

4. val_acc_{dataset}.png
- Most important plot - validation accuracy with grokking points marked
- Vertical dashed lines show step when 99% validation accuracy was reached
- Labels indicate which run each grokking point belongs to
- Clear differences in grokking speed between tasks and runs

5. train_val_acc_{dataset}_{run}.png (e.g. train_val_acc_x_plus_y_run_1.png)
- Compares training vs validation accuracy for each run/dataset combination
- Shows generalization gap before and after grokking
- Vertical dotted line marks grokking point
- Particularly useful for analyzing x_plus_y which shows unusual train-val dynamics
