{
    "Name": "attention_evolution_analysis",
    "Title": "Evolution of Attention Patterns in Character-Level Language Models",
    "Problem Statement": "How do attention patterns in character-level language models evolve during training to capture different linguistic structures across diverse datasets?",
    "Motivation": "Understanding how attention mechanisms naturally adapt to different linguistic styles could reveal fundamental learning dynamics and inform more efficient architecture designs that leverage emergent patterns.",
    "Proposed Method": "Track and analyze the evolution of attention patterns across layers and training iterations, comparing models trained on different datasets (shakespeare_char, enwik8). Develop metrics to quantify attention span, consistency, and hierarchical structure formation.",
    "Experiment": "1) Modify CausalSelfAttention to track attention weights during training. 2) Add visualization and analysis tools for attention patterns. 3) Train models on different datasets. 4) Compute attention metrics (entropy, span length, cross-layer similarity) at regular intervals. 5) Correlate pattern evolution with validation loss and generation quality.",
    "Interestingness": 8,
    "Feasibility": 9,
    "Novelty": 7,
    "lit_review_results": "title: What is the best recipe for character-level encoder-only modelling?\nabstract: This paper aims to benchmark recent progress in language understanding models that output contextualised representations at the character level. Many such modelling architectures and methods to train those architectures have been proposed, but it is currently unclear what the relative contributions of the architecture vs. the pretraining objective are to final model performance. We explore the design space of such models, comparing architectural innovations and a variety of different pretraining objectives on a suite of evaluation tasks with a fixed training procedure in order to find the currently optimal way to build and train character-level BERT-like models. We find that our best performing character-level model exceeds the performance of a token-based model trained with the same settings on the same data, suggesting that character-level models are ready for more widespread adoption. Unfortunately, the best method to train character-level models still relies on a subword-level tokeniser during pretraining, and final model performance is highly dependent on tokeniser quality. We believe our results demonstrate the readiness of character-level models for multilingual language representation, and encourage NLP practitioners to try them as drop-in replacements for token-based models.\n\ntitle: TinyHelen's First Curriculum: Training and Evaluating Tiny Language   Models in a Simpler Language Environment\nabstract: Training language models (LMs) and their application agents is increasingly costly due to large datasets and models, making test failures difficult to bear. Simplified language environments serve as primordial training and testing grounds, retaining essential commonsense and communication skills but in a more digestible form, potentially enhancing the learning efficiency of LMs, and thus reducing the required model size and data volume for effective training and evaluation. In these simplified language environments, workable strategies for small models, datasets, and agents may be adaptable to larger models, datasets, and agents in complex language environments.   To create such environments, we focus on two aspects: i) minimizing language dataset noise and complexity, and ii) preserving the essential text distribution characteristics. Unlike previous methods, we propose a pipeline to refine text data by eliminating noise, minimizing vocabulary, and maintaining genre-specific patterns (e.g., for books, conversation, code, etc.). Implementing this pipeline with large LMs, we have created a leaner suite of LM training and evaluation datasets: 71M Leaner-Pretrain, 7M Leaner-Instruct, Leaner-Glue for assessing linguistic proficiency, and Leaner-Eval for testing instruction-following ability.   Our experiments show that leaner pre-training boosts LM learning efficiency. Tiny LMs trained on these datasets outperform those trained on original datasets in instruction-following across different language granularity levels. Moreover, the Leaner-Pretrain dataset's alignment with conventional large LM training sets enables resource-optimized analysis of how learning objectives, model architectures, and training techniques impact performance on language modeling and downstream tasks. Our code and datasets are available at https://github.com/EmpathYang/TinyHelen.git.\n\ntitle: Toucan: Token-Aware Character Level Language Modeling\nabstract: Character-level language models obviate the need for separately trained tokenizers, but efficiency suffers from longer sequence lengths. Learning to combine character representations into tokens has made training these models more efficient, but they still require decoding characters individually. We propose Toucan, an augmentation to character-level models to make them \"token-aware\". Comparing our method to prior work, we demonstrate significant speed-ups in character generation without a loss in language modeling performance. We then explore differences between our learned dynamic tokenization of character sequences with popular fixed vocabulary solutions such as Byte-Pair Encoding and WordPiece, finding our approach leads to a greater amount of longer sequences tokenized as single items. Our project and code are available at https://nlp.jhu.edu/nuggets/.\n\ntitle: CharBERT: Character-aware Pre-trained Language Model\nabstract: Most pre-trained language models (PLMs) construct word representations at subword level with Byte-Pair Encoding (BPE) or its variations, by which OOV (out-of-vocab) words are almost avoidable. However, those methods split a word into subword units and make the representation incomplete and fragile. In this paper, we propose a character-aware pre-trained language model named CharBERT improving on the previous methods (such as BERT, RoBERTa) to tackle these problems. We first construct the contextual word embedding for each token from the sequential character representations, then fuse the representations of characters and the subword representations by a novel heterogeneous interaction module. We also propose a new pre-training task named NLM (Noisy LM) for unsupervised character representation learning. We evaluate our method on question answering, sequence labeling, and text classification tasks, both on the original datasets and adversarial misspelling test sets. The experimental results show that our method can significantly improve the performance and robustness of PLMs simultaneously. Pretrained models, evaluation sets, and code are available at https://github.com/wtma/CharBERT\n\ntitle: TinyStories: How Small Can Language Models Be and Still Speak Coherent   English?\nabstract: Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention).   In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities.   We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency.   We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.\n\n",
    "paper": "title: STYLEFUSION: ADAPTIVE MULTI-STYLE GENERATION IN CHARACTER-LEVEL LANGUAGE MODELS\nabstract: This paper introduces the Multi-Style Adapter, a novel approach to enhance style awareness and consistency in character-level language models. As language models advance, the ability to generate text in diverse and consistent styles becomes crucial for applications ranging from creative writing assistance to personalized content generation. However, maintaining style consistency while preserving language generation capabilities presents a significant challenge. Our Multi-Style Adapter addresses this by introducing learnable style embeddings and a style classification head, working in tandem with a StyleAdapter module to modulate the hidden states of a transformer-based language model. We implement this approach by modifying the GPT architecture, incorporating style adaptation after every transformer layer to create stronger style-specific representations. Through extensive experiments on multiple datasets, including Shakespeareâ€™s works (shakespeare_char), enwik8, and text8, we demonstrate that our approach achieves high style consistency while maintaining competitive language modeling performance. Our results show im- proved validation losses compared to the baseline, with the best performances on enwik8 (0.9488) and text8 (0.9145). Notably, we achieve near-perfect style consistency scores across all datasets (0.9667 for shakespeare_char, 1.0 for enwik8 and text8). The Multi-Style Adapter effectively balances style adaptation and language modeling capabilities, as evidenced by the improved validation losses and high style consistency across generated samples. However, this comes at a cost of increased computational complexity, resulting in slower inference speeds (approximately 400 tokens per second compared to 670 in the baseline). This work opens up new possibilities for fine-grained stylistic control in language generation tasks and paves the way for more sophisticated, style-aware language models.",
    "paper_review_results": "{\"paper_id\": \"StyleFusion: Adaptive Multi-style Generation in Character-Level Language Models\", \"Summary\": \"This paper proposes the Multi-Style Adapter, an architectural extension to transformer-based character-level language models (specifically GPT) to enable style-aware and style-consistent text generation. The method introduces learnable style embeddings, a style classification head, and adapter modules after each transformer layer to modulate hidden states based on inferred style. Experiments are conducted on shakespeare_char, enwik8, and text8 datasets, reporting minor improvements in validation loss and high 'style consistency' as measured by a classifier, but with a significant reduction in inference speed.\", \"Questions\": \"[\\\"How are 'styles' defined and labeled for each dataset, especially for enwik8 and text8? Are these datasets actually multi-style, or are styles artificially imposed?\\\", 'How is the style consistency metric computed? How is the classifier trained, and what are its accuracy and reliability? Is it robust to non-synthetic, real-world texts?', \\\"Can you provide qualitative examples of generated text for different styles to demonstrate the model's control and diversity?\\\", 'Why are strong baselines (e.g., CTRL, AdapterFusion, style-token methods) not included in the experimental comparison?', 'Does the model generalize to unseen styles or only to those seen during training?', 'Are there any human studies or external validation of the claimed stylistic control?', 'What do the learned style embeddings correspond to? Are they interpretable?', 'What is the practical implication of the reduction in inference speed? Is the approach scalable to larger models and datasets?']\", \"Limitations\": \"['The main limitation is the absence of a rigorous definition and labeling of style, making the experimental claims unverifiable.', 'The evaluation of style consistency is unclear and potentially tautological.', 'No qualitative or human evaluation is provided.', 'No comparison to strong style-control baselines.', 'Significant computational cost for modest or ambiguous gains.', 'Possible overfitting or lack of diversity in generated outputs.', 'Limited applicability beyond character-level models.', 'No discussion of potential negative societal impacts or ethical concerns.']\", \"Ethical Concerns\": false, \"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Overall\": 2, \"Confidence\": 4, \"Strengths\": \"['Addresses the relevant and practical problem of stylistic control in language generation.', 'The proposed method is modular, simple, and compatible with existing transformer architectures.', 'The paper is generally clearly written and includes ablation studies and quantitative results.', 'Implementation details are provided, and the approach is easy to integrate into existing models.']\", \"Weaknesses\": \"[\\\"The definition and operationalization of 'style' are not specified, especially for enwik8 and text8; it is unclear how styles are defined, labeled, or distinguished, making experimental claims unverifiable.\\\", 'The style consistency metric relies on a classifier trained on synthetic data, but the construction, validation, and reliability of this classifier are not described, raising concerns about tautological or trivial results.', 'No qualitative analysis or human evaluation is provided to demonstrate that the model produces genuinely stylistic or diverse outputs.', 'Improvements in validation loss are marginal or negative (e.g., on shakespeare_char), while inference speed is significantly reduced (~40% slower).', 'No comparison is made to strong, relevant baselines from the style control literature (e.g., CTRL, AdapterFusion, style-token methods).', 'Technical novelty is limited; the approach is an incremental combination of existing ideas (adapters, style embeddings, classifiers).', 'No evidence is provided for generalization to unseen styles, diversity within styles, or practical value in downstream tasks.', 'Ethical and societal impacts of style manipulation are not discussed.', 'Crucial experimental and methodological details are omitted, making the work difficult to evaluate or reproduce.']\", \"Originality\": 2, \"Quality\": 2, \"Clarity\": 3, \"Significance\": 2, \"Decision\": \"Reject\"}",
    "novel": true
}