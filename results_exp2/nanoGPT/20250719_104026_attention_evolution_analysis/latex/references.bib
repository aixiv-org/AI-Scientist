%% LaTeX2e file `references.bib'
%% generated by the `filecontents' environment
%% from source `template' on 2025/07/19.
%%
@article{lu2024aiscientist,
  title={The {AI} {S}cientist: Towards Fully Automated Open-Ended Scientific Discovery},
  author={Lu, Chris and Lu, Cong and Lange, Robert Tjarko and Foerster, Jakob and Clune, Jeff and Ha, David},
  journal={arXiv preprint arXiv:2408.06292},
  year={2024}
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  year={2016},
  publisher={MIT Press}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{karpathy2023nanogpt,
  title = {nanoGPT},
  author = {Karpathy, Andrej},
  year = {2023},
  journal = {URL https://github.com/karpathy/nanoGPT/tree/master},
  note = {GitHub repository}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{loshchilov2017adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@misc{gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  year={2024},
  eprint={2303.08774},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2303.08774},
}

@Article{Bensemann2022EyeGA,
 author = {Joshua Bensemann and A. Peng and Diana Benavides Prado and Yang Chen and N. Tan and P. Corballis and Patricia Riddle and Michael Witbrock},
 booktitle = {Workshop on Cognitive Modeling and Computational Linguistics},
 journal = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},
 title = {Eye Gaze and Self-attention: How Humans and Transformers Attend Words in Sentences},
 year = {2022}
}


@Article{Voita2019AnalyzingMS,
 author = {Elena Voita and David Talbot and F. Moiseev and Rico Sennrich and Ivan Titov},
 booktitle = {Annual Meeting of the Association for Computational Linguistics},
 journal = {ArXiv},
 title = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
 volume = {abs/1905.09418},
 year = {2019}
}


@Article{Clark2019WhatDB,
 author = {Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
 booktitle = {BlackboxNLP@ACL},
 pages = {276-286},
 title = {What Does BERT Look at? An Analysis of BERTâ€™s Attention},
 year = {2019}
}


@Article{Michel2019AreSH,
 author = {Paul Michel and Omer Levy and Graham Neubig},
 booktitle = {Neural Information Processing Systems},
 journal = {ArXiv},
 title = {Are Sixteen Heads Really Better than One?},
 volume = {abs/1905.10650},
 year = {2019}
}


@Article{Dong2021AttentionIN,
 author = {Yihe Dong and Jean-Baptiste Cordonnier and Andreas Loukas},
 booktitle = {International Conference on Machine Learning},
 journal = {ArXiv},
 title = {Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth},
 volume = {abs/2103.03404},
 year = {2021}
}


@Article{Al-Rfou2018CharacterLevelLM,
 author = {Rami Al-Rfou and Dokook Choe and Noah Constant and Mandy Guo and Llion Jones},
 booktitle = {AAAI Conference on Artificial Intelligence},
 pages = {3159-3166},
 title = {Character-Level Language Modeling with Deeper Self-Attention},
 year = {2018}
}


@Article{Chen2024TrainingDO,
 author = {Siyu Chen and Heejune Sheen and Tianhao Wang and Zhuoran Yang},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality},
 volume = {abs/2402.19442},
 year = {2024}
}


@Article{Chen2024TrainingDO,
 author = {Siyu Chen and Heejune Sheen and Tianhao Wang and Zhuoran Yang},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality},
 volume = {abs/2402.19442},
 year = {2024}
}


@Article{Tian2023JoMADM,
 author = {Yuandong Tian and Yiping Wang and Zhenyu (Allen) Zhang and Beidi Chen and Simon S. Du},
 booktitle = {International Conference on Learning Representations},
 journal = {ArXiv},
 title = {JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention},
 volume = {abs/2310.00535},
 year = {2023}
}

