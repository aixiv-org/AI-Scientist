\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Clark2019WhatDB,Voita2019AnalyzingMS}
\citation{bahdanau2014neural}
\citation{goodfellow2016deep}
\citation{karpathy2023nanogpt}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}}
\citation{paszke2019pytorch}
\citation{Clark2019WhatDB,Voita2019AnalyzingMS}
\citation{Michel2019AreSH}
\citation{Al-Rfou2018CharacterLevelLM}
\citation{Dong2021AttentionIN,Tian2023JoMADM}
\citation{Chen2024TrainingDO}
\citation{karpathy2023nanogpt}
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related}{{2}{2}{Related Work}{section.2}{}}
\newlabel{sec:related@cref}{{[section][2][]2}{[1][2][]2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Static Attention Analysis}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Character-Level Modeling}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Training Dynamics Theory}{2}{subsection.2.3}\protected@file@percent }
\citation{karpathy2023nanogpt}
\citation{bahdanau2014neural}
\citation{paszke2019pytorch}
\citation{ba2016layer}
\citation{loshchilov2017adamw}
\citation{karpathy2023nanogpt}
\citation{paszke2019pytorch}
\@writefile{toc}{\contentsline {section}{\numberline {3}Background}{3}{section.3}\protected@file@percent }
\newlabel{sec:background}{{3}{3}{Background}{section.3}{}}
\newlabel{sec:background@cref}{{[section][3][]3}{[1][2][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Transformer Architecture}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Character-Level Modeling}{3}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Problem Setting}{3}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Attention Metrics}{3}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Method}{3}{section.4}\protected@file@percent }
\newlabel{sec:method}{{4}{3}{Method}{section.4}{}}
\newlabel{sec:method@cref}{{[section][4][]4}{[1][3][]3}}
\citation{loshchilov2017adamw}
\citation{ba2016layer}
\citation{karpathy2023nanogpt}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Attention Pattern Tracking}{4}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Attention Metrics}{4}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Implementation}{4}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Setup}{4}{section.5}\protected@file@percent }
\newlabel{sec:experimental}{{5}{4}{Experimental Setup}{section.5}{}}
\newlabel{sec:experimental@cref}{{[section][5][]5}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Datasets}{4}{subsection.5.1}\protected@file@percent }
\citation{ba2016layer}
\citation{loshchilov2017adamw}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Model Configuration}{5}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Training Protocol}{5}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Attention Tracking}{5}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Evaluation}{5}{subsection.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{5}{section.6}\protected@file@percent }
\newlabel{sec:results}{{6}{5}{Results}{section.6}{}}
\newlabel{sec:results@cref}{{[section][6][]6}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Training Dynamics}{5}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Attention Pattern Evolution}{5}{subsection.6.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:val_loss}{{1a}{6}{Validation loss convergence\relax }{figure.caption.1}{}}
\newlabel{fig:val_loss@cref}{{[subfigure][1][1]1a}{[1][5][]6}}
\newlabel{sub@fig:val_loss}{{a}{6}{Validation loss convergence\relax }{figure.caption.1}{}}
\newlabel{sub@fig:val_loss@cref}{{[subfigure][1][1]1a}{[1][5][]6}}
\newlabel{fig:train_loss}{{1b}{6}{Training loss optimization\relax }{figure.caption.1}{}}
\newlabel{fig:train_loss@cref}{{[subfigure][2][1]1b}{[1][5][]6}}
\newlabel{sub@fig:train_loss}{{b}{6}{Training loss optimization\relax }{figure.caption.1}{}}
\newlabel{sub@fig:train_loss@cref}{{[subfigure][2][1]1b}{[1][5][]6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Training curves showing minimal impact of attention tracking (0.8\% overhead). Similar patterns observed across all datasets.\relax }}{6}{figure.caption.1}\protected@file@percent }
\newlabel{fig:training_curves}{{1}{6}{Training curves showing minimal impact of attention tracking (0.8\% overhead). Similar patterns observed across all datasets.\relax }{figure.caption.1}{}}
\newlabel{fig:training_curves@cref}{{[figure][1][]1}{[1][5][]6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Attention metrics evolution showing phase transitions (dashed lines). Top: Entropy (bits). Bottom: Effective span (tokens). Patterns consistent across datasets.\relax }}{6}{figure.caption.2}\protected@file@percent }
\newlabel{fig:attention_metrics}{{2}{6}{Attention metrics evolution showing phase transitions (dashed lines). Top: Entropy (bits). Bottom: Effective span (tokens). Patterns consistent across datasets.\relax }{figure.caption.2}{}}
\newlabel{fig:attention_metrics@cref}{{[figure][2][]2}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Layer Specialization}{7}{subsection.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Layer similarity (JSD) evolution. Middle layers (4-6) maintain consistent patterns while final layers specialize.\relax }}{7}{figure.caption.3}\protected@file@percent }
\newlabel{fig:layer_similarity}{{3}{7}{Layer similarity (JSD) evolution. Middle layers (4-6) maintain consistent patterns while final layers specialize.\relax }{figure.caption.3}{}}
\newlabel{fig:layer_similarity@cref}{{[figure][3][]3}{[1][7][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Generation Quality}{7}{subsection.6.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Attention metrics correlate with generation quality (n-gram diversity and repetition scores). Values from \texttt  {shakespeare\_char} dataset.\relax }}{7}{table.caption.4}\protected@file@percent }
\newlabel{tab:generation}{{1}{7}{Attention metrics correlate with generation quality (n-gram diversity and repetition scores). Values from \texttt {shakespeare\_char} dataset.\relax }{table.caption.4}{}}
\newlabel{tab:generation@cref}{{[table][1][]1}{[1][7][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Limitations}{7}{subsection.6.5}\protected@file@percent }
\citation{vaswani2017attention}
\citation{paszke2019pytorch}
\citation{lu2024aiscientist}
\bibstyle{iclr2024_conference}
\bibdata{references}
\bibcite{Al-Rfou2018CharacterLevelLM}{{1}{2018}{{Al-Rfou et~al.}}{{Al-Rfou, Choe, Constant, Guo, and Jones}}}
\bibcite{ba2016layer}{{2}{2016}{{Ba et~al.}}{{Ba, Kiros, and Hinton}}}
\bibcite{bahdanau2014neural}{{3}{2014}{{Bahdanau et~al.}}{{Bahdanau, Cho, and Bengio}}}
\bibcite{Chen2024TrainingDO}{{4}{2024}{{Chen et~al.}}{{Chen, Sheen, Wang, and Yang}}}
\bibcite{Clark2019WhatDB}{{5}{2019}{{Clark et~al.}}{{Clark, Khandelwal, Levy, and Manning}}}
\bibcite{Dong2021AttentionIN}{{6}{2021}{{Dong et~al.}}{{Dong, Cordonnier, and Loukas}}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions and Future Work}{8}{section.7}\protected@file@percent }
\newlabel{sec:conclusion}{{7}{8}{Conclusions and Future Work}{section.7}{}}
\newlabel{sec:conclusion@cref}{{[section][7][]7}{[1][8][]8}}
\bibcite{goodfellow2016deep}{{7}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, Courville, and Bengio}}}
\bibcite{karpathy2023nanogpt}{{8}{2023}{{Karpathy}}{{}}}
\bibcite{loshchilov2017adamw}{{9}{2017}{{Loshchilov \& Hutter}}{{Loshchilov and Hutter}}}
\bibcite{lu2024aiscientist}{{10}{2024}{{Lu et~al.}}{{Lu, Lu, Lange, Foerster, Clune, and Ha}}}
\bibcite{Michel2019AreSH}{{11}{2019}{{Michel et~al.}}{{Michel, Levy, and Neubig}}}
\bibcite{paszke2019pytorch}{{12}{2019}{{Paszke et~al.}}{{Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.}}}
\bibcite{Tian2023JoMADM}{{13}{2023}{{Tian et~al.}}{{Tian, Wang, Zhang, Chen, and Du}}}
\bibcite{vaswani2017attention}{{14}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{Voita2019AnalyzingMS}{{15}{2019}{{Voita et~al.}}{{Voita, Talbot, Moiseev, Sennrich, and Titov}}}
\ttl@finishall
\gdef \@abspage@last{9}
