\begin{thebibliography}{15}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Al-Rfou et~al.(2018)Al-Rfou, Choe, Constant, Guo, and
  Jones]{Al-Rfou2018CharacterLevelLM}
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones.
\newblock Character-level language modeling with deeper self-attention.
\newblock pp.\  3159--3166, 2018.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem[Chen et~al.(2024)Chen, Sheen, Wang, and Yang]{Chen2024TrainingDO}
Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang.
\newblock Training dynamics of multi-head softmax attention for in-context
  learning: Emergence, convergence, and optimality.
\newblock \emph{ArXiv}, abs/2402.19442, 2024.

\bibitem[Clark et~al.(2019)Clark, Khandelwal, Levy, and
  Manning]{Clark2019WhatDB}
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher~D. Manning.
\newblock What does bert look at? an analysis of bertâ€™s attention.
\newblock pp.\  276--286, 2019.

\bibitem[Dong et~al.(2021)Dong, Cordonnier, and Loukas]{Dong2021AttentionIN}
Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas.
\newblock Attention is not all you need: Pure attention loses rank doubly
  exponentially with depth.
\newblock \emph{ArXiv}, abs/2103.03404, 2021.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
\newblock \emph{Deep learning}, volume~1.
\newblock MIT Press, 2016.

\bibitem[Karpathy(2023)]{karpathy2023nanogpt}
Andrej Karpathy.
\newblock nanogpt.
\newblock \emph{URL https://github.com/karpathy/nanoGPT/tree/master}, 2023.
\newblock GitHub repository.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017adamw}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Lu et~al.(2024)Lu, Lu, Lange, Foerster, Clune, and
  Ha]{lu2024aiscientist}
Chris Lu, Cong Lu, Robert~Tjarko Lange, Jakob Foerster, Jeff Clune, and David
  Ha.
\newblock The {AI} {S}cientist: Towards fully automated open-ended scientific
  discovery.
\newblock \emph{arXiv preprint arXiv:2408.06292}, 2024.

\bibitem[Michel et~al.(2019)Michel, Levy, and Neubig]{Michel2019AreSH}
Paul Michel, Omer Levy, and Graham Neubig.
\newblock Are sixteen heads really better than one?
\newblock \emph{ArXiv}, abs/1905.10650, 2019.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Tian et~al.(2023)Tian, Wang, Zhang, Chen, and Du]{Tian2023JoMADM}
Yuandong Tian, Yiping Wang, Zhenyu~(Allen) Zhang, Beidi Chen, and Simon~S. Du.
\newblock Joma: Demystifying multilayer transformers via joint dynamics of mlp
  and attention.
\newblock \emph{ArXiv}, abs/2310.00535, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Voita et~al.(2019)Voita, Talbot, Moiseev, Sennrich, and
  Titov]{Voita2019AnalyzingMS}
Elena Voita, David Talbot, F.~Moiseev, Rico Sennrich, and Ivan Titov.
\newblock Analyzing multi-head self-attention: Specialized heads do the heavy
  lifting, the rest can be pruned.
\newblock \emph{ArXiv}, abs/1905.09418, 2019.

\end{thebibliography}
