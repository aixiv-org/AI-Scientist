%% LaTeX2e file `references.bib'
%% generated by the `filecontents' environment
%% from source `template' on 2025/07/19.
%%
@article{lu2024aiscientist,
  title={The {AI} {S}cientist: Towards Fully Automated Open-Ended Scientific Discovery},
  author={Lu, Chris and Lu, Cong and Lange, Robert Tjarko and Foerster, Jakob and Clune, Jeff and Ha, David},
  journal={arXiv preprint arXiv:2408.06292},
  year={2024}
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  year={2016},
  publisher={MIT Press}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{karpathy2023nanogpt,
  title = {nanoGPT},
  author = {Karpathy, Andrej},
  year = {2023},
  journal = {URL https://github.com/karpathy/nanoGPT/tree/master},
  note = {GitHub repository}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{loshchilov2017adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@misc{gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  year={2024},
  eprint={2303.08774},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2303.08774},
}

@Article{Rousseeuw1987SilhouettesAG,
 author = {P. Rousseeuw},
 journal = {Journal of Computational and Applied Mathematics},
 pages = {53-65},
 title = {Silhouettes: a graphical aid to the interpretation and validation of cluster analysis},
 volume = {20},
 year = {1987}
}


@Article{Sutskever2011GeneratingTW,
 author = {I. Sutskever and James Martens and Geoffrey E. Hinton},
 booktitle = {International Conference on Machine Learning},
 pages = {1017-1024},
 title = {Generating Text with Recurrent Neural Networks},
 year = {2011}
}


@Article{Al-Rfou2018CharacterLevelLM,
 author = {Rami Al-Rfou and Dokook Choe and Noah Constant and Mandy Guo and Llion Jones},
 booktitle = {AAAI Conference on Artificial Intelligence},
 pages = {3159-3166},
 title = {Character-Level Language Modeling with Deeper Self-Attention},
 year = {2018}
}


@Article{Clark2019WhatDB,
 author = {Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
 booktitle = {BlackboxNLP@ACL},
 pages = {276-286},
 title = {What Does BERT Look at? An Analysis of BERTâ€™s Attention},
 year = {2019}
}


@Article{Voita2019AnalyzingMS,
 author = {Elena Voita and David Talbot and F. Moiseev and Rico Sennrich and Ivan Titov},
 booktitle = {Annual Meeting of the Association for Computational Linguistics},
 journal = {ArXiv},
 title = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
 volume = {abs/1905.09418},
 year = {2019}
}


@Article{Rogers2020API,
 author = {Anna Rogers and Olga Kovaleva and Anna Rumshisky},
 booktitle = {Transactions of the Association for Computational Linguistics},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {842-866},
 title = {A Primer in BERTology: What We Know About How BERT Works},
 volume = {8},
 year = {2020}
}


@Article{Hwang2016CharacterlevelLM,
 author = {Kyuyeon Hwang and Wonyong Sung},
 booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},
 journal = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
 pages = {5720-5724},
 title = {Character-level language modeling with hierarchical recurrent neural networks},
 year = {2016}
}

