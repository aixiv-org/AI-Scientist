# Title: Emergent Hierarchical Representations in Character-Level Language Models
# Experiment description: 1) Modify the GPT class to store hidden states during forward passes. 2) Add analysis functions to compute similarity matrices and clustering metrics. 3) Train models on shakespeare_char and enwik8 datasets. 4) Track how hidden state patterns evolve and correlate with validation loss. 5) Compare patterns between layers and datasets to understand representation development.

## Plots and Analysis:

1. Training Loss Plots (train_loss_*.png):
   - Shows training loss curves for each dataset (shakespeare_char, enwik8, text8)
   - Key insights: Compare convergence rates between datasets and runs
   - File pattern: train_loss_[dataset].png

2. Validation Loss Plots (val_loss_*.png):
   - Shows validation loss curves with error bands
   - Key insights: Model generalization and overfitting patterns
   - File pattern: val_loss_[dataset].png

3. Cross-Layer Similarity (cross_layer_similarity.png):
   - Shows cosine similarity between consecutive layers' hidden states
   - Key insights: How information transforms through network hierarchy
   - Reveals layer specialization patterns over training

4. Attention Head Specialization (attention_specialization.png):
   - Shows attention head similarity within layers
   - Key insights: Degree of attention head specialization
   - Lower values indicate more specialized attention patterns

5. Cluster Quality Analysis (cluster_quality.png):
   - Shows silhouette scores for hidden state clusters
   - Key insights: Measures how well-defined clusters are in hidden space
   - Higher scores indicate more distinct representations

6. Training Dynamics Summary (training_dynamics.png):
   - Composite plot showing both training and validation loss
   - Key insights: Overall model learning trajectory
   - Shows relationship between training and validation performance

## Run 0: Baseline
Results: {'shakespeare_char': {'final_train_loss_mean': 0.8060949246088663, 'best_val_loss_mean': 1.4682018359502156, 'total_train_time_mean': 142.441192706426, 'avg_inference_tokens_per_second_mean': 420.0450710143821}, 'enwik8': {'final_train_loss_mean': 0.9472248554229736, 'best_val_loss_mean': 1.0063371658325195, 'total_train_time_mean': 1210.4159588813782, 'avg_inference_tokens_per_second_mean': 423.8404041909007}, 'text8': {'final_train_loss_mean': 1.0053755044937134, 'best_val_loss_mean': 0.980750560760498, 'total_train_time_mean': 1189.628823518753, 'avg_inference_tokens_per_second_mean': 410.90266803396463}}
Description: Baseline results with no hidden state analysis. Used for comparison with experimental runs.
